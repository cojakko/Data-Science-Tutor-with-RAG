{"query": "What is the difference between supervised and unsupervised learning?", "relevant_passages": ["374 10. Unsupervised Learning\ngrasp of supervised learning. For instance, if you are asked to predict a\nbinary outcome from a data set, you have a very well developed set of tools\nat your disposal (such as logistic regression, linear discriminant analysis,\nclassiﬁcation trees, support vector machines, and more) as well as a clear\nunderstanding of how to assess the quality of the results obtained (using\ncross-validation, validation on an independent test set, and so forth).\nIn contrast, unsupervised learning is often much more challenging. The\nexercise tends to be more subjective, and there is no simple goal for the\nanalysis, such as prediction of a response. Unsupervised learning is often\nperformed as part of anexploratory data analysis.F u r t h e r m o r e ,i tc a nb eexploratory\ndata analysishard to assess the results obtained from unsupervised learning methods,\nsince there is no universally accepted mechanism for performing cross-", "book.\n2.1.4 Supervised Versus Unsupervised Learning\nMost statistical learning problems fall into one of two categories:supervised supervised\nor unsupervised. The examples that we have discussed so far in this chap-unsupervised\nter all fall into the supervised learning domain. For each observation of the\npredictor measurement(s) xi, i =1 ,...,n there is an associated response\nmeasurement yi. We wish to ﬁt a model that relates the response to the\npredictors, with the aim of accurately predicting the response for future\nobservations (prediction) or better understanding the relationship between\nthe response and the predictors (inference). Many classical statistical learn-\ning methods such as linear regressionandlogistic regression(Chapter 4), aslogistic\nregressionwell as more modern approaches such as GAM, boosting, and support vec-\ntor machines, operate in the supervised learning domain. The vast majority\nof this book is devoted to this setting.", "we discover subgroups among the variables or among the observations?\nUnsupervised learning refers to a diverse set of techniques for answering\nquestions such as these. In this chapter, we will focus on two particu-\nlar types of unsupervised learning:principal components analysis,at o o l\nused for data visualization or data pre-processing before supervised tech-\nniques are applied, andclustering, a broad class of methods for discovering\nunknown subgroups in data.\n10.1 The Challenge of Unsupervised Learning\nSupervised learning is a well-understood area. In fact, if you have read\nthe preceding chapters in this book, then you should by now have a good\nG. James et al.,An Introduction to Statistical Learning: with Applications in R,\nSpringer Texts in Statistics 103, DOI 10.1007/978-1-4614-7138-710,\n© Springer Science+Business Media New York 2013\n373", "tor machines, operate in the supervised learning domain. The vast majority\nof this book is devoted to this setting.\nIn contrast, unsupervised learning describes the somewhat more chal-\nlenging situation in which for every observationi =1 ,...,n ,w eo b s e r v e\na vector of measurementsxi but no associated responseyi.I ti sn o tp o s -\nsible to ﬁt a linear regression model,since there is no response variable\nto predict. In this setting, we are in some sense working blind; the sit-\nuation is referred to as unsupervised b e c a u s ew el a c kar e s p o n s ev a r i -\nable that can supervise our analysis. What sort of statistical analysis is"]}
{"query": "Explain the bias–variance tradeoff in simple terms.", "relevant_passages": ["The bias-variance tradeoff arises because increasing the model’s complexity can reduce the bias but increase the variance. On the other hand, decreasing the complexity can reduce the variance but increase the bias. The goal is to find the optimal balance between bias and variance, which results in the best generalization performance on new, unseen data.", "The bias-variance tradeoff is a fundamental and widely discussed concept in the area of Data Science. Understanding the bias-variance tradeoff is essential for developing accurate and reliable machine learning models, as it can help us optimize model performance and avoid common pitfalls such as underfitting and overfitting.\n\nBefore defining it, it is necessary to define what is bias and variance separately.\nBias\nBias refers to the error that is introduced by approximating a real-life problem with a simplified model. A model with high bias is not able to capture the true complexity of the data and tends to underfit, leading to poor performance on both the training and test data. The bias is represented by the difference between the expected or true value of the target variable and the predicted value of the model.", "Variance\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training data. A model with high variance tends to overfit the training data, leading to poor performance on new, unseen data. Variance is represented by the degree of variability or spreads in the model’s predictions for different training sets.\n\nUnderstanding the bias-variance tradeoff is essential for developing accurate and reliable Machine Learning models. It can help to optimize the model performance and avoid common pitfalls such as underfitting and overfitting. One of the best ways to visualize the bias and variance concepts is through a dartboard like the one shown below.\n\nSource: V. Gudivada, A. Apon & J. Ding, 2017\nSource: V. Gudivada, A. Apon & J. Ding, 2017\nThe figure shows how variance and bias are related:", "This is directly related to the complexity of the model used, as shown in the figure below.\n\nBias-variance tradeoff and error relationship (image by author)\nBias-variance tradeoff and error relationship (image by author)\nThe graph shows how the complexity of the model is related to the values of bias and variance. Models that have a low complexity can be too simple to understand the patterns of the data used in the training, a phenomenon called underfitting. **** Consequently, it won’t be able to make good predictions on the test data, resulting in a high bias."]}
{"query": "How does regularization help prevent overfitting?", "relevant_passages": ["Chapter 9\nRegularization and model\nselection\n9.1 Regularization\nRecall that as discussed in Section 8.1, overftting is typically a result of using\ntoo complex models, and we need to choose a proper model complexity to\nachieve the optimal bias-variance tradeoﬀ. When the model complexity is\nmeasured by the number of parameters, we can vary the size of the model\n(e.g., the width of a neural net). However, the correct, informative complex-\nity measure of the models can be a function of the parameters (e.g., ℓ2 norm\nof the parameters), which may not necessarily depend on the number of pa-\nrameters. In such cases, we will use regularization, an important technique\nin machine learning, control the model complexity and prevent overﬁtting.\nRegularization typically involves adding an additional term, called a reg-\nularizer and denoted by R(θ) here, to the training loss/cost function:\nJλ(θ) = J(θ) + λR(θ) (9.1)\nHere Jλ is often called the regularized loss, and λ ≥0 is called the regular-", "137\nThe R(θ) = ∥θ∥1 (also called LASSO) and R(θ) = 1\n2 ∥θ∥2\n2 are perhaps\namong the most commonly used regularizers for linear models. Other norm\nand powers of norms are sometimes also used. The ℓ2 norm regularization is\nmuch more commonly used with kernel methods because ℓ1 regularization is\ntypically not compatible with the kernel trick (the optimal solution cannot\nbe written as functions of inner products of features.)\nIn deep learning, the most commonly used regularizer is ℓ2 regularization\nor weight decay. Other common ones include dropout, data augmentation,\nregularizing the spectral norm of the weight matrices, and regularizing the\nLipschitzness of the model, etc. Regularization in deep learning is an ac-\ntive research area, and it’s known that there is another implicit source of\nregularization, as discussed in the next section.\n9.2 Implicit regularization eﬀect (optional\nreading)\nThe implicit regularization eﬀect of optimizers, or implicit bias or algorithmic", "then applying the standard gradient\nθ←θ−η∇Jλ(θ) = θ−ηλθ−η∇J(θ)\n= (1 −λη)θ  \ndecaying weights\n−η∇J(θ) (9.2)\nBesides encouraging simpler models, regularization can also impose in-\nductive biases or structures on the model parameters. For example, suppose\nwe had a prior belief that the number of non-zeros in the ground-truth model\nparameters is small,2—which is oftentimes called sparsity of the model—, we\ncan impose a regularization on the number of non-zeros in θ, denoted by\n∥θ∥0, to leverage such a prior belief. Imposing additional structure of the\nparameters narrows our search space and makes the complexity of the model\nfamily smaller,—e.g., the family of sparse models can be thought of as having\nlower complexity than the family of all models—, and thus tends to lead to a\nbetter generalization. On the other hand, imposing additional structure may\nrisk increasing the bias. For example, if we regularize the sparsity strongly"]}
{"query": "What is the purpose of a validation set?", "relevant_passages": ["see, the key concepts remain the same regardless of whether the response\nis quantitative or qualitative.\n5.1.1 The Validation Set Approach\nSuppose that we would like to estimate the test error associated with ﬁt-\nting a particular statistical learning method on a set of observations. The\nvalidation set approach, displayed in Figure 5.1, is a very simple strategyvalidation\nset approachfor this task. It involves randomly dividing the available set of observa-\ntions into two parts, atraining setand avalidation setor hold-out set.T h evalidation\nset\nhold-out set\nmodel is ﬁt on the training set, and the ﬁtted model is used to predict the\nresponses for the observationsin the validation set. The resulting validation\nset error rate—typically assessed using MSE in the case of a quantitative\nresponse—provides an estimate of the test error rate.\nWeillustratethevalidationsetapproachonthe Auto dataset.Recallfrom\nChapter 3 that there appears to be a non-linear relationship betweenmpg", "cisely which observations are included in the training set and which\nobservations are included in the validation set.\n2. In the validation approach, only a subset of the observations—those\nthat are included in the training set rather than in the validation\nset—are used to ﬁt the model. Since statistical methods tend to per-\nform worse when trained on fewer observations, this suggests that the\nvalidation set error rate may tend tooverestimate the test error rate\nfor the model ﬁt on the entire data set.\nIn the coming subsections, we will presentcross-validation, a reﬁnement of\nthe validation set approach that addresses these two issues.\n5.1.2 Leave-One-Out Cross-Validation\nLeave-one-out cross-validation(LOOCV) is closely relatedto the validationleave-one-\nout\ncross-\nvalidation\nset approach of Section 5.1.1, but it attempts to address that method’s\ndrawbacks.\nLike the validation set approach, LOOCV involves splitting the set of", "1. Randomly split S into Strain (say, 70% of the data) and Scv (the remain-\ning 30%). Here, Scv is called the hold-out cross validation set.\n2. Train each model Mi on Strain only, to get some hypothesis hi.\n3. Select and output the hypothesis hi that had the smallest error ˆεScv (hi)\non the hold out cross validation set. (Here ˆ εScv (h) denotes the average\nerror of h on the set of examples in Scv.) The error on the hold out\nvalidation set is also referred to as the validation error.\nBy testing/validating on a set of examples Scv that the models were not\ntrained on, we obtain a better estimate of each hypothesis hi’s true general-\nization/test error. Thus, this approach is essentially picking the model with\nthe smallest estimated generalization/test error. The size of the validation\nset depends on the total number of available examples. Usually, somewhere\nbetween 1/4−1/3 of the data is used in the hold out cross validation set, and"]}
{"query": "When would you prefer Gradient Boosting over Random Forests?", "relevant_passages": ["8.2 Bagging, Random Forests, Boosting 323\n0 1000 2000 3000 4000 5000\n0.05 0.10 0.15 0.20 0.25\nNumber of Trees\nTest Classification Error\nBoosting: depth=1\nBoosting: depth=2\nRandomForest: m= p\nFIGURE 8.11. Results from performing boosting and random forests on the\n15-class gene expression data set in order to predictcancer versus normal.T h e\ntest error is displayed as a function of the number of trees. For the two boosted\nmodels, λ =0 .01. Depth-1 trees slightly outperform depth-2 trees, and both out-\nperform the random forest, although the standard errors are around 0.02, making\nnone of these diﬀerences signiﬁcant. The test error rate for a single tree is 24%.\nWe have just described the process of boosting regression trees. Boosting\nclassiﬁcation trees proceeds in a similar but slightly more complex way, and\nthe details are omitted here.\nBoosting has three tuning parameters:\n1. The number of treesB. Unlike bagging and random forests, boosting", "stumps with an interaction depth of one perform well if enough of them\nare included. This model outperforms the depth-two model, and both out-\nperform a random forest. This highlights one diﬀerence between boosting\nand random forests: in boosting, because the growth of a particular tree\ntakes into account the other trees that have already been grown, smaller\ntrees are typically suﬃcient. Using smaller trees can aid in interpretability\nas well; for instance, using stumps leads to an additive model.\n8.3 Lab: Decision Trees\n8.3.1 Fitting Classiﬁcation Trees\nThe tree library is used to construct classiﬁcation and regression trees.\n> library(tree)"]}
{"query": "Explain the intuition behind cross-validation.", "relevant_passages": ["11.1.3  Leave-one-out cross-validation and  caret \nThe idea behind cross-validation is similar to that behind test-training splitting of the data. We\npartition the data into several sets and use one of them for evaluation. The key difference is\nthat in a cross-validation we partition the data into more than two sets and use all of them\n(one-by-one) for evaluation.\nTo begin with, we split the data into  sets, where  is equal to or less than the number of\nobservations . We then put the first set aside, to use for evaluation, and fit the model to the\nremaining  sets. The model predictions are then evaluated on the first set. Next, we put\nthe first set back among the others and remove the second set to use that for evaluation. And\nso on. This means that we fit  models to  different (albeit similar) training sets, and evaluate\nthem on  test sets (none of which are used for fitting the model that is evaluated on them).", "from the past. Or, more generally, new data won’t be quite the same as old data. What we do,\nin a lot of situations, is try to derive mathematical rules that help us to draw the inferences that\nare most likely to be correct for new data, rather than to pick the statements that best describe\nold data. For instance, given two models A and B, and a data set X you collected today, try to\npick the model that will best describe a new data set Y that you’re going to collect tomorrow.\nSometimes it’s convenient to simulate the process, and that’s what cross-validation does. What you\ndo is divide your data set into two subsets, X1 and X2. Use the subset X1 to train the model (e.g.,\nestimate regression coeﬃcients, let’s say), but then assess the model performance on the other one\nX2. This gives you a measure of how well the model generalises from an old data set to a new one,\nand is often a better measure of how good your model is than if you just ﬁt it to the full data set\nX.", "then computed on the observations in the held-out fold. This procedure is\nrepeated k times; each time, a diﬀerent group of observations is treated\nas a validation set. This process results ink estimates of the test error,\nMSE1,MSE2,..., MSEk.T h ek-fold CV estimate is computed by averaging\nthese values,\nCV(k) = 1\nk\nk∑\ni=1\nMSEi. (5.3)\nFigure 5.5 illustrates thek-fold CV approach.\nIt is not hardto see that LOOCV is a specialcase ofk-foldCV in whichk\nis set to equaln. In practice, one typically performsk-fold CV usingk =5\nor k = 10. What is the advantage of usingk =5o r k = 10 rather than\nk = n? The most obvious advantage is computational. LOOCV requires\nﬁtting the statistical learning methodn times. This has the potential to be\ncomputationally expensive (except for linear models ﬁt by least squares,\nin which case formula (5.2) can be used). But cross-validation is a very\ngeneral approach that can be applied to almost any statistical learning"]}
{"query": "Compare batch gradient descent and stochastic gradient descent.", "relevant_passages": ["This algorithm is called stochastic gradient descent (also incremental\ngradient descent). Whereas batch gradient descent has to scan through\nthe entire training set before taking a single step—a costly operation if n is\nlarge—stochastic gradient descent can start making progress right away, and", "13\ncontinues to make progress with each example it looks at. Often, stochastic\ngradient descent gets θ “close” to the minimum much faster than batch gra-\ndient descent. (Note however that it may never “converge” to the minimum,\nand the parameters θ will keep oscillating around the minimum of J(θ); but\nin practice most of the values near the minimum will be reasonably good\napproximations to the true minimum.2) For these reasons, particularly when\nthe training set is large, stochastic gradient descent is often preferred over\nbatch gradient descent.\n1.2 The normal equations\nGradient descent gives one way of minimizing J. Let’s discuss a second way\nof doing so, this time performing the minimization explicitly and without\nresorting to an iterative algorithm. In this method, we will minimize J by\nexplicitly taking its derivatives with respect to the θj’s, and setting them to\nzero. To enable us to do this without having to write reams of algebra and", "11\nBy grouping the updates of the coordinates into an update of the vector\nθ, we can rewrite update (1.1) in a slightly more succinct way:\nθ:= θ+ α\nn∑\ni=1\n(\ny(i) −hθ(x(i))\n)\nx(i)\nThe reader can easily verify that the quantity in the summation in the\nupdate rule above is just ∂J(θ)/∂θj (for the original deﬁnition of J). So, this\nis simply gradient descent on the original cost function J. This method looks\nat every example in the entire training set on every step, and is called batch\ngradient descent. Note that, while gradient descent can be susceptible\nto local minima in general, the optimization problem we have posed here\nfor linear regression has only one global, and no other local, optima; thus\ngradient descent always converges (assuming the learning rate α is not too\nlarge) to the global minimum. Indeed, J is a convex quadratic function.\nHere is an example of gradient descent as it is run to minimize a quadratic\nfunction.\n5 10 15 20 25 30 35 40 45 50\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50"]}
{"query": "What is transfer learning and when is it useful?", "relevant_passages": ["178\neven no labeled data. The intuition is that the pretrained models learn good\nrepresentations that capture intrinsic semantic structure/ information about\nthe data, and the adaptation phase customizes the model to a particular\ndownstream task by, e.g., retrieving the information speciﬁc to it. For ex-\nample, a model pretrained on massive unlabeled image data may learn good\ngeneral visual representations/features, and we adapt the representations to\nsolve biomedical imagining tasks.\nWe formalize the two phases below.\nPretraining. Suppose we have an unlabeled pretraining dataset\n{x(1),x(2) ··· ,x(n)}that consists of nexamples in Rd. Let φθ be a model that\nis parameterized by θand maps the input xto some m-dimensional represen-\ntation φθ(x). (People also call φθ(x) ∈Rm the embedding or features of the\nexample x.) We pretrain the model θ with a pretraining loss, which is often\nan average of loss functions on all the examples:Lpre(θ) = 1\nn\n∑n\ni=1 ℓpre(θ,x(i)).", "setting is called few-shot learning. It’s also pretty common to have a larger\nntask on the order of ranging from hundreds to tens of thousands.\nAn adaptation algorithm generally takes in a downstream dataset and the\npretrained model ˆθ, and outputs a variant of ˆθ that solves the downstream\ntask. We will discuss below two popular and general adaptation methods,\nlinear probe and ﬁnetuning. In addition, two other methods speciﬁc to lan-\nguage problems are introduced in 14.3.2.\nThe linear probe approach uses a linear head on top of the representation\nto predict the downstream labels. Mathematically, the adapted model out-\nputs w⊤φˆθ(x), where w ∈Rm is a parameter to be learned, and ˆθ is exactly\nthe pretrained model (ﬁxed). We can use SGD (or other optimizers) to train"]}
{"query": "Describe how attention mechanisms work in modern neural networks.", "relevant_passages": ["sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query , keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key .\n3.2.1 Scaled Dot-Product Attention\nW e call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. W e compute the dot products of the\n3", "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √ dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously , packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . W e compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n√ dk\n)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√ dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity , dot-product attention is", "block, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,"]}
{"query": "What is the role of activation functions in deep learning?", "relevant_passages": ["91\nFigure 7.3: Activation functions in deep learning.\nSuppose σ(z) = z, then for two-layer neural network, we have that\n¯hθ(x) = W[2]a[1] (7.29)\n= W[2]σ(z[1]) by deﬁnition (7.30)\n= W[2]z[1] since σ(z) = z (7.31)\n= W[2]W[1]x from Equation (7.18) (7.32)\n= ˜Wx where ˜W = W[2]W[1] (7.33)\nNotice how W[2]W[1] collapsed into ˜W.\nThis is because applying a linear function to another linear function will\nresult in a linear function over the original input (i.e., you can construct a ˜W\nsuch that ˜Wx = W[2]W[1]x). This loses much of the representational power\nof the neural network as often times the output we are trying to predict\nhas a non-linear relationship with the inputs. Without non-linear activation\nfunctions, the neural network will simply perform linear regression.\nConnection to the Kernel Method. In the previous lectures, we covered\nthe concept of feature maps. Recall that the main motivation for feature\nmaps is to represent functions that are non-linear in the input x by θ⊤φ(x),"]}
{"query": "How do you handle class imbalance in a classification problem?", "relevant_passages": ["correct predictions 90% or 99.99% of the time? Are there things that the model shouldn’t be\nallowed to take into account, such as skin colour or income? If so, how can you make sure that\nsuch variables aren’t implicitly incorporated into the training data?\n11.3  Challenges in predictive modelling\nThere are a number of challenges that often come up in predictive modelling projects. In this\nsection we’ll briefly discuss some of them.\n11.3.1  Handling class imbalance\nImbalanced data, where the proportions of different classes differ a lot, are common in\npractice. In some areas, such as the study of rare diseases, such datasets are inherent to the\nfield. Class imbalance can cause problems for many classifiers, as they tend to become prone\nto classify too many observations as belonging to the more common class.\nOne way to mitigate this problem is to use down-sampling and up-sampling when fitting the", "One way to mitigate this problem is to use down-sampling and up-sampling when fitting the\nmodel. In down-sampling, only a (random) subset of the observations from the larger class are\nused for fitting the model, so that the number of cases from each class becomes balanced. In\nup-sampling, the number of observations in the smaller class are artificially increased by\nresampling, also to achieve balance. These methods are only used when fitting the model, to\navoid problems with the model overfitting to the class imbalance.\nTo illustrate the need and use for these methods, let’s create a more imbalanced version of the\n wine  data:\nNext, we fit three logistic models – one the usual way, one with down-sampling, and one with\nup-sampling. We’ll use 10-fold cross-validation to evaluate their performance.\n# Create imbalanced wine data:\nwine_imb <- wine[1:5000,]\n# Check class balance:\ntable(wine_imb$type)\n05/11/25, 11:09 11 Predictive modelling and machine learning | Modern Statistics with R"]}
{"query": "Explain how SHAP values help interpret ML models.", "relevant_passages": ["7: Return bias metrics for analysis \n \n4.3 Explainable AI (XAI) for Model Transparency \nUnderstanding AI decision-making is critical to building trust. Two widely used interpretability techniques, SHAP and LIME, help \nexplain how individual features contribute to model predictions. \n• Shapley Additive Explanations (SHAP): This technique assigns importance scores to each feature, helping users understand \nwhich inputs influence a model’s decisions. Algorithm 2 describes how SHAP values are computed and used to create an \nexplanatory visualization. \n \nAlgorithm 2 SHAP Explanation Algorithm \n \n1: Input: Trained model, dataset \n2: Output: Feature importance scores \n3: Train the model on the given dataset \n4: Initialize SHAP explainer \n5: Compute feature importance scores based on model predictions \n6: Return SHAP summary visualization \n \n• Local Interpretable Model-Agnostic Explanations (LIME): LIME simplifies complex AI models by approximat-ing them", "• SHAP (Shapley Additive Explanat ions): Assigns feature importance scores to highlight which variables influence AI \npredictions [30]. \n• LIME (Local Interpretable Model-Agnostic Explanations): Generates local approximations of complex AI models to explain \nindividual decisions [18]. \n• Interpretable Neural Networks : Incorporating attention -based models to highlight the most relevant input features in deep \nlearning applications [16]. \nXAI techniques are especially critical in high-risk applications such as healthcare, finance, and criminal justice, where transparency \nensures fairness and regulatory compliance [13]. Frameworks like the EU AI Act and General Data Protection Regulation (GDPR) \nmandate explainability to protect individual rights, reinforcing the need for interpretable AI models [17]. \n6.3 Strengthening Data Protection Laws and Privacy Regulations"]}
{"query": "What is the curse of dimensionality and how do you mitigate it?", "relevant_passages": ["dimension has only caused a small deterioration in the linear regressiontest\nset MSE, but it has caused more than a ten-fold increase in the MSE for\nKNN. This decreasein performanceas the dimension increasesis a common\nproblem for KNN, and results from the fact that in higher dimensions\nthere is eﬀectively a reduction in sample size. In this data set there are\n100 training observations; whenp = 1, this provides enough information to\naccurately estimatef(X).However,spreading 100observations over p =2 0\ndimensions results in a phenomenon in which a given observation has no\nnearby neighbors—this is the so-called curse of dimensionality.T h a ti s ,curse of di-\nmensionalitythe K observations that are nearest to a given test observationx0 may be\nvery far away fromx0 in p-dimensional space whenp is large, leading to a", "ﬁnd a low-dimensional representation of the data that captures as much of\ntheinformationaspossible.Forinstance,ifwecanobtainatwo-dimensional\nrepresentation of the data that captures most of the information, then we\ncan plot the observations in this low-dimensional space.\nPCA provides a tool to do just this. It ﬁnds a low-dimensional represen-\ntation of a data set that contains as much as possible of the variation. The\nidea is that each of then observations lives inp-dimensional space, but not\nall of these dimensions are equally interesting. PCA seeks a small number\nof dimensions that are as interesting as possible, where the concept ofin-\nteresting is measured by the amount that the observations vary along each\ndimension. Each of the dimensions found by PCA is a linear combination\nof thep features. We now explain the manner in which these dimensions,\nor principal components, are found.\nThe ﬁrst principal componentof a set of featuresX1,X2,...,X p is the"]}
{"query": "What is the difference between generative and discriminative models?", "relevant_passages": ["Chapter 4\nGenerative learning algorithms\nSo far, we’ve mainly been talking about learning algorithms that model\np(y|x; θ), the conditional distribution of y given x. For instance, logistic\nregression modeled p(y|x; θ) as hθ(x) = g(θTx) where g is the sigmoid func-\ntion. In these notes, we’ll talk about a diﬀerent type of learning algorithm.\nConsider a classiﬁcation problem in which we want to learn to distinguish\nbetween elephants ( y = 1) and dogs ( y = 0), based on some features of\nan animal. Given a training set, an algorithm like logistic regression or\nthe perceptron algorithm (basically) tries to ﬁnd a straight line—that is, a\ndecision boundary—that separates the elephants and dogs. Then, to classify\na new animal as either an elephant or a dog, it checks on which side of the\ndecision boundary it falls, and makes its prediction accordingly.\nHere’s a diﬀerent approach. First, looking at elephants, we can build a", "try to model p(x|y) (and p(y)). These algorithms are called generative\nlearning algorithms. For instance, if y indicates whether an example is a\ndog (0) or an elephant (1), then p(x|y = 0) models the distribution of dogs’\nfeatures, and p(x|y= 1) models the distribution of elephants’ features.\nAfter modeling p(y) (called the class priors) and p(x|y), our algorithm\n34"]}
{"query": "How does a transformer architecture differ from an RNN?", "relevant_passages": ["tion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2, 16]. In all but a few cases [ 22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,", "3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively .\n3.1 Encoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2", "Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. W e employ a residual connection [ 10] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. T o facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. W e also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This"]}
{"query": "Describe the main steps involved in building an ML pipeline for production.", "relevant_passages": ["system lifecycle, including organisations  and individuals that deploy or operate AI.  An AI system lifecycle \ntypically involves several phases that include to: plan and design; collect and process data; build model(s) \nand/or adapt existing model(s) to specific tasks; test, evaluate, verify and val idate; make available for \nuse/deploy; operate and monitor; and retire/decommission. These phases often take place in an iterative \nmanner and are not necessarily sequential. The decision to retire an AI system from operation may occur \nat any point during the operation and monitoring phase.  \nGenerative AI \nGenerative artificial intelligence (AI) systems create new content (e.g.  text, image, audio, or video) in \nresponse to prompts, based on the data the models have been trained on. Generative AI is based on \nmachine learning (ML), which has developed gradually since the 1950s. ML models leverage deep neural", "| **Decision workflows**  | [Flows](/en/guides/flows/first-flow) | When you need predictable, auditable decision paths with precise control                                                                    |\n| **API orchestration**   | [Flows](/en/guides/flows/first-flow) | For reliable integration with multiple external services in a specific sequence                                                             |\n| **Hybrid applications** | Combined approach                    | Use [Flows](/en/guides/flows/first-flow) to orchestrate overall process with [Crews](/en/guides/crews/first-crew) handling complex subtasks |"]}
{"query": "What is the difference between correlation and causation?", "relevant_passages": ["And SciPy provides a similar function called spearmanr, because rank correlation is also\ncalled Spearman’s correlation.\nAs an exercise, you’ll have a chance to compute the correlation between SAT verbal scores\nand income, using both Pearson correlation and rank correlation.\n7.6. Correlation and Causation\nIf variables A and B are correlated, the apparent correlation might be due to random\nsampling, or it might be the result of non-representative sampling, or it might indicate a real\ncorrelation between quantities in the population.\nIf the correlation is real, there are three possible explanations: A causes B, or B causes A, or\nsome other set of factors causes both A and B. These explanations are called “causal\nrelationships”.\nCorrelation alone does not distinguish between these explanations, so it does not tell you\nwhich ones are true. This rule is often summarized with the phrase “Correlation does not\nimply causation,” which is so pithy it has its own Wikipedia page.", "imply causation,” which is so pithy it has its own Wikipedia page.\nhttp://wikipedia.org/wiki/Correlation_does_not_imply_causation\nSo what can you do to provide evidence of causation?\n1. Use time. If A comes before B, then A can cause B but not the other way around. The\norder of events can help us infer the direction of causation, but it does not preclude the\npossibility that something else causes both A and B.\n2. Use randomness. If you divide a large sample into two groups at random and compute\nthe means of almost any variable, you expect the difference to be small. If the groups\nare nearly identical in all variables but A and B, you can rule out the possibility that\nsomething else causes both A and B.\nThese ideas are the motivation for the randomized controlled trial, in which subjects are\nassigned randomly to two (or more) groups: a treatment group that receives some kind of\nfrom scipy.stats import spearmanr\nspearmanr(valid[\"piat_math\"], valid[\"income\"]).statistic", "a ﬁring squad. We might want to measure whether or not an execution order was given (variable\nA), whether or not a marksman ﬁred their gun (variable B), and whether or not the person got hit\nwith a bullet (variable C). These three variables are all correlated with one another (e.g., there is\na correlation between guns being ﬁred and people getting hit with bullets), but we actually want\nto make stronger statements about them than merely talking about correlations. We want to talk\nabout causation. We want to be able to say that the execution order (A) causes the marksman to\nﬁre (B) which causes someone to get shot (C). We can express this by a directed arrow notation:\nwe write it as AÑB ÑC. This “causal chain” is a fundamentally diﬀerent explanation for events\nthan one in which the marksman ﬁres ﬁrst, which causes the shooting B Ñ C, and then causes\nthe executioner to “retroactively” issue the execution order, B ÑA. This “common eﬀect” model", "two variables, like we saw with the ﬁgures in the section on correlation (Section 5.7). It’s this latter\napplication that we usually have in mind when we use the term “scatterplot”. In this kind of plot, each\nobservation corresponds to one dot: the horizontal location of the dot plots the value of the observation\non one variable, and the vertical location displays its value on the other variable. In many situations you\ndon’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B\ncause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter\nwhich variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations\nyou do have a pretty strong idea which variable you think is most likely to be causal, or at least you have\nsome suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis,", "surveys, review medical or company records, or follow a cohort of many similar individuals to form\nhypotheses about why certain diseases might develop. In each of these situations, researchers merely\nobserve the data that arise. In general, observational studies can provide evidence of a naturally\noccurring association between variables, but they cannot by themselves show a causal connection.\nWhen researchers want to investigate the possibility of a causal connection, they conduct an\nexperiment. Usually there will be both an explanatory and a response variable. For instance, we\nmay suspect administering a drug will reduce mortality in heart attack patients over the following\nyear. To check if there really is a causal connection between the explanatory variable and the\nresponse, researchers will collect a sample of individuals and split them into groups. The individuals\nin each group are assigned a treatment. When individuals are randomly assigned to a group, the"]}
{"query": "Explain the Central Limit Theorem in simple terms.", "relevant_passages": ["abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of\nthis mathematical law, the normal distribution pops up over and over again in real data.\n4As usual, I’m being a bit sloppy here. The central limit theorem is a bit more general than this section implies. Like\nmost introductory stats texts, I’ve discussed one situation where the central limit theorem holds: when you’re taking an\naverage across lots of independent events drawn from the same distribution. However, the central limit theorem is much\nbroader than this. There’s a whole class of things called “ U-statistics” for instance, all of which satisfy the central limit\ntheorem and therefore become normally distributed for large sample sizes. The mean is one such statistic, but it’s not the\nonly one.\n- 313 -", "world. Many characteristics of living things are affected by genetic and environmental factors\nwhose effect is additive. The characteristics we measure are the sum of a large number of\nsmall effects, so their distribution tends to be normal.\nTo see how the Central Limit Theorem works, and when it doesn’t, let’s try some\nexperiments, starting with an exponential distribution. The following loop generates samples\nfrom an exponential distribution, adds them up, and makes a dictionary that maps from each\nsample size, n, to a list of 1001 sums.\n05/11/25, 10:24 14. Analytic Methods — Think Stats, 3rd edition\nhttps://allendowney.github.io/ThinkStats/chap14.html 17/40", "sum converges to normal as n increases. More specifically, if the distribution of the values\nhas mean m and variance s2 the distribution of the sum converges to a normal distribution\nwith mean n * m and variance n * s2.\nThat conclusion is the Central Limit Theorem (CLT). It is one of the most useful tools for\nstatistical analysis, but it comes with caveats:\nThe values have to come from the same distribution (although this requirement can be\nrelaxed).\nThe values have to be drawn independently. If they are correlated, the CLT doesn’t apply\n(although it can still work if the correlation is not too strong).\nThe values have to be drawn from a distribution with finite mean and variance. So the\nCLT doesn’t apply to some long-tailed distributions.\nThe Central Limit Theorem explains the prevalence of normal distributions in the natural\nworld. Many characteristics of living things are affected by genetic and environmental factors", "172 CHAPTER 5. FOUNDATIONS FOR INFERENCE\nSample Proportions\n0.84 0.86 0.88 0.90 0.92\n0\n250\n500\n750\nFrequency\nFigure 5.2: A histogram of 10,000 sample proportions, where each sample is taken\nfrom a population where the population proportion is 0.88 and the sample size is\nn = 1000.\n5.1.3 Central Limit Theorem\nThe distribution in Figure 5.2 looks an awful lot like a normal distribution. That is no anomaly;\nit is the result of a general principle called the Central Limit Theorem.\nCENTRAL LIMIT THEOREM AND THE SUCCESS-FAILURE CONDITION\nWhen observations are independent and the sample size is sufficiently large, the sample propor-\ntion ˆp will tend to follow a normal distribution with the following mean and standard error:\nµˆp = p SE ˆp =\nr\np(1 − p)\nn\nIn order for the Central Limit Theorem to hold, the sample size is typically considered sufficiently\nlarge when np ≥ 10 and n(1 − p) ≥ 10, which is called the success-failure condition."]}
{"query": "How do confidence intervals differ from prediction intervals?", "relevant_passages": ["the 95% prediction interval is [7,930, 14,580]. We interpret this to mean\nthat 95% of intervals of this form will contain the true value ofY for this\ncity. Note thatboth intervalsare centeredat 11,256,butthat the prediction\ninterval is substantially wider than the conﬁdence interval, reﬂecting the\nincreased uncertainty about sales for a given city in comparison to the\naverage sales over many locations.\n3.3 Other Considerations in the Regression Model\n3.3.1 Qualitative Predictors\nIn our discussion so far, we have assumed that all variables in our linear\nregression model are quantitative. But in practice, this is not necessarily\nthe case; often some predictors arequalitative.\n8In other words, if we collect a large number of data sets like theAdvertising data\nset, and we construct a conﬁdence interval for the averagesales on the basis of each\ndata set (given $100,000 in TV and $20,000 in radio advertising), then 95% of these", "the 95% prediction interval is [7,930, 14,580]. We interpret this to mean\nthat 95% of intervals of this form will contain the true value ofY for this\ncity. Note thatboth intervalsare centeredat 11,256,butthat the prediction\ninterval is substantially wider than the conﬁdence interval, reﬂecting the\nincreased uncertainty about sales for a given city in comparison to the\naverage sales over many locations.\n3.3 Other Considerations in the Regression Model\n3.3.1 Qualitative Predictors\nIn our discussion so far, we have assumed that all variables in our linear\nregression model are quantitative. But in practice, this is not necessarily\nthe case; often some predictors arequalitative.\n8In other words, if we collect a large number of data sets like theAdvertising data\nset, and we construct a conﬁdence interval for the averagesales on the basis of each\ndata set (given $100,000 in TV and $20,000 in radio advertising), then 95% of these", "We are 95% confident that with each dollar increase in family income, the university’s gift aid is\npredicted to decrease on average by $0.0214 to $0.0648.\nCONFIDENCE INTERVALS FOR COEFFICIENTS\nConfidence intervals for model coefficients can be computed using the t-distribution:\nbi ± t⋆\nd f× SEbi\nwhere t⋆\nd fis the appropriate t-value corresponding to the confidence level with the model’s\ndegrees of freedom.\nOn the topic of intervals in this book, we’ve focused exclusively on confidence intervals for\nmodel parameters. However, there are other types of intervals that may be of interest, including\nprediction intervals for a response value and also confidence intervals for a mean response value in\nthe context of regression. These two interval types are introduced in an online extra that you may\ndownload at\nwww.openintro.org/d?file=stat extra linear regression supp", "normality. Prediction intervals provide interval estimates for the new observations. They\nincorporate both the uncertainty associated with our model estimates, and the fact that the\nnew observation is likely to deviate slightly from its expected value. To get 90% prediction\nintervals, we add  interval = \"prediction\"  and  level = 0.9 :\nIf we were using a transformed -variable, we’d probably have to transform the predictions\nback to the original scale for them to be useful:\n∼\nnew_cars <- data.frame(hp = c(161, 612), wt = c(4.473, 3.462),\n                       row.names = c(\"Volvo XC90\", \"Ferrari Roma\"))\npredict(m, new_cars)\nm <- lm(mpg ~ hp + wt, data = mtcars)\npredict(m, new_cars,\n        interval = \"prediction\",\n        level = 0.9)\ny\n05/11/25, 11:08 8 Regression models | Modern Statistics with R\nhttps://www.modernstatisticswithr.com/regression.html 20/86", "[4] \"rank\" \"fitted.values\" \"assign\"\n[7] \"qr\" \"df.residual\" \"xlevels\"\n[10] \"call\" \"terms\" \"model\"\n> coef(lm.fit)\n(Intercept ) lstat\n34.55 -0.95\nIn order to obtain a conﬁdence interval for the coeﬃcient estimates, we can\nuse theconfint() command. confint()\n> confint(lm.fit)\n2.5 % 97.5 %\n(Intercept ) 33.45 35.659\nlstat -1.03 -0.874\nThe predict() function can be used to produce conﬁdence intervals andpredict()\nprediction intervals for the prediction ofmedv for a given value oflstat.\n> predict(lm.fit,data.frame(lstat=c(5,10 ,15)),\ninterval =\"confidence \")\nfit lwr upr\n1 29.80 29.01 30.60\n2 25.05 24.47 25.63\n3 20.30 19.73 20.87"]}
{"query": "What assumptions underlie linear regression?", "relevant_passages": ["The standard linear regression model (3.19) provides interpretable results\nand works quite well on many real-world problems. However, it makes sev-\neral highly restrictive assumptions that are often violated in practice. Two\nof the most important assumptions state that the relationship between the\npredictors and response areadditive and linear. The additive assumptionadditive\nlinear", "regression model. We further explore this approach and other non-linear\nextensions of the linear model in Chapter 7.\n3.3.3 Potential Problems\nWhen we ﬁt a linear regression model to a particular data set, many prob-\nlems may occur. Most common among these are the following:\n1. Non-linearity of the response-predictor relationships.\n2. Correlation of error terms.\n3. Non-constant variance of error terms.\n4. Outliers.\n5. High-leverage points.\n6. Collinearity.\nIn practice, identifying and overcoming these problems is as much an\nart as a science. Many pages in countless books have been written on this\ntopic. Since the linear regression model is not our primary focus here, we\nwill provide only a brief summary of some key points.\n1. Non-linearity of the Data\nThe linear regression model assumes that there is a straight-line relation-\nship between the predictors and the response. If the true relationship is\nfar from linear, then virtually all of the conclusions that we draw from the", "far from linear, then virtually all of the conclusions that we draw from the\nﬁt are suspect. In addition, the prediction accuracy of the model can be\nsigniﬁcantly reduced.\nResidual plots are a useful graphical tool for identifying non-linearity.residual plot\nGiven a simple linear regression model, we can plot the residuals,ei =\nyi − ˆyi, versus the predictorxi. In the case of a multiple regression model,"]}
{"query": "When would you use a non-parametric test instead of a parametric one?", "relevant_passages": ["approach brings with it the possibility that the functional form used to\nestimate f is very diﬀerent from the truef, in which case the resulting\nmodel will not ﬁt the data well. In contrast, non-parametric approaches\ncompletely avoid this danger, since essentially no assumption about the\nform off is made. But non-parametric approaches do suﬀer from a major\ndisadvantage: since they do not reduce the problem of estimatingf to a\nsmall number of parameters, a very large number of observations (far more\nthan is typically needed for a parametric approach) is required in order to\nobtain an accurate estimate forf.\nAn example of a non-parametric approach to ﬁtting theIncome data is\ns h o w ni nF i g u r e2 . 5 .Athin-plate spline is used to estimate f. This ap- thin-plate\nsplineproach does not impose any pre-speciﬁed model onf. It instead attempts\nto produce an estimate forf that is as close as possible to the observed\ndata, subject to the ﬁt—that is, the yellow surface in Figure 2.5—being", "approach brings with it the possibility that the functional form used to\nestimate f is very diﬀerent from the truef, in which case the resulting\nmodel will not ﬁt the data well. In contrast, non-parametric approaches\ncompletely avoid this danger, since essentially no assumption about the\nform off is made. But non-parametric approaches do suﬀer from a major\ndisadvantage: since they do not reduce the problem of estimatingf to a\nsmall number of parameters, a very large number of observations (far more\nthan is typically needed for a parametric approach) is required in order to\nobtain an accurate estimate forf.\nAn example of a non-parametric approach to ﬁtting theIncome data is\ns h o w ni nF i g u r e2 . 5 .Athin-plate spline is used to estimate f. This ap- thin-plate\nsplineproach does not impose any pre-speciﬁed model onf. It instead attempts\nto produce an estimate forf that is as close as possible to the observed\ndata, subject to the ﬁt—that is, the yellow surface in Figure 2.5—being", "gression, the coeﬃcients have simple interpretations, and tests of statistical\nsigniﬁcance can be easily performed. But parametric methods do have a\ndisadvantage: by construction, they make strong assumptions about the\nform of f(X). If the speciﬁed functional form is far from the truth, and\nprediction accuracy is our goal, then the parametric method will perform\npoorly. For instance, if we assumea linear relationship betweenX and Y\nbut the true relationship is far from linear, then the resulting model will\nprovide a poor ﬁt to the data, and any conclusions drawn from it will be\nsuspect.\nIn contrast, non-parametric methods do not explicitly assume a para-\nmetric form forf(X), and thereby provide an alternative and more ﬂexi-\nble approach for performing regression. We discuss various non-parametric\nmethods in this book. Here we consider one of the simplest and best-known\nnon-parametricmethods, K-nearest neighbors regression(KNNregression). K-nearest\nneighbors\nregression", "gression, the coeﬃcients have simple interpretations, and tests of statistical\nsigniﬁcance can be easily performed. But parametric methods do have a\ndisadvantage: by construction, they make strong assumptions about the\nform of f(X). If the speciﬁed functional form is far from the truth, and\nprediction accuracy is our goal, then the parametric method will perform\npoorly. For instance, if we assumea linear relationship betweenX and Y\nbut the true relationship is far from linear, then the resulting model will\nprovide a poor ﬁt to the data, and any conclusions drawn from it will be\nsuspect.\nIn contrast, non-parametric methods do not explicitly assume a para-\nmetric form forf(X), and thereby provide an alternative and more ﬂexi-\nble approach for performing regression. We discuss various non-parametric\nmethods in this book. Here we consider one of the simplest and best-known\nnon-parametricmethods, K-nearest neighbors regression(KNNregression). K-nearest\nneighbors\nregression"]}
{"query": "What is the p-value and why is it often misunderstood?", "relevant_passages": ["would not correspond to the probability that the null is true; this interpretation is entirely inconsistent\nwith the mathematics of how the p value is calculated. Put bluntly, despite the intuitive appeal of\nthinking this way, there is no justiﬁcation for interpreting a p value this way. Never do it.\n11.6\nReporting the results of a hypothesis test\nWhen writing up the results of a hypothesis test, there’s usually several pieces of information that you\nneed to report, but it varies a fair bit from test to test. Throughout the rest of the book I’ll spend a\nlittle time talking about how to report the results of diﬀerent tests (see Section 12.1.9 for a particularly\ndetailed example), so that you can get a feel for how it’s usually done. However, regardless of what test\nyou’re doing, the one thing that you always have to do is say something about the p value, and whether\nor not the outcome was signiﬁcant.", "extreme, if the null hypothesis were true. To find the p-value, we generally find the null distribution,\nand then we find a tail area in that distribution corresponding to our point estimate.", "0.37 0.50\nObserved p^ = 0.37\nFigure 5.9: If the null hypothesis were true, this normal distribution describes the\ndistribution of ˆp.\nCHECKING SUCCESS-FAILURE AND COMPUTING SEˆ pFOR A HYPOTHESIS TEST\nWhen using the p-value method to evaluate a hypothesis test, we check the conditions for ˆp and\nconstruct the standard error using the null value, p0, instead of using the sample proportion.\nIn a hypothesis test with a p-value, we are supposing the null hypothesis is true, which is a\ndifferent mindset than when we compute a confidence interval. This is why we use p0 instead\nof ˆp when we check conditions and compute the standard error in this context.\nWhen we identify the sampling distribution under the null hypothesis, it has a special name:\nthe null distribution. The p-value represents the probability of the observed ˆp, or a ˆp that is more\nextreme, if the null hypothesis were true. To find the p-value, we generally find the null distribution,", "applications. Another problem is that this use of “significant” is misleading because it\nsuggests that the effect is important in practice. The correlation between mother’s age and\nbirth weight is a good example – it is statistically significant, but so small that it is not\nimportant.\nAn alternative is to interpret p-values qualitatively.\nIf a p-value is large, it is plausible that the observed effect could happen by chance.\ncompute_p_value(simulated_corrs, observed_corr)\nnp.float64(0.0)\n05/11/25, 10:23 9. Hypothesis Testing — Think Stats, 3rd edition\nhttps://allendowney.github.io/ThinkStats/chap09.html 12/19", "hardly unusual: in my experience, most practitioners express views very similar to Fisher’s. In essence,\nthe pă.05 convention is assumed to represent a fairly stringent evidentiary standard.\nWell, how true is that? One way to approach this question is to try to convert p-values to Bayes\nfactors, and see how the two compare. It’s not an easy thing to do because a p-value is a fundamentally\ndiﬀerent kind of calculation to a Bayes factor, and they don’t measure the same thing. However, there\nhave been some attempts to work out the relationship between the two, and it’s somewhat surprising. For\nexample, Johnson (2013) presents a pretty compelling case that (for t-tests at least) the pă.05 threshold\ncorresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 in favour of the alternative. If\nthat’s right, then Fisher’s claim is a bit of a stretch. Let’s suppose that the null hypothesis is true about"]}
{"query": "Explain Type I and Type II errors.", "relevant_passages": ["Figure 5.8: Four different scenarios for hypothesis tests.\nA Type 1 Error is rejecting the null hypothesis when H0 is actually true. A Type 2 Error\nis failing to reject the null hypothesis when the alternative is actually true.\nGUIDED PRACTICE 5.25\nIn a US court, the defendant is either innocent ( H0) or guilty ( HA). What does a Type 1 Error\nrepresent in this context? What does a Type 2 Error represent? Figure 5.8 may be useful. 19\nEXAMPLE 5.26\nHow could we reduce the Type 1 Error rate in US courts? What influence would this have on the\nType 2 Error rate?\nTo lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable\ndoubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this\nwould also make it more difficult to convict the people who are actually guilty, so we would make\nmore Type 2 Errors.\nGUIDED PRACTICE 5.27\nHow could we reduce the Type 2 Error rate in US courts? What influence would this have on the", "(c) What is a Type 2 Error in this context?\n5.26 Which is higher? In each part below, there is a value of interest and two scenarios (I and II). For\neach part, report if the value of interest is larger under scenario I, scenario II, or whether the value is equal\nunder the scenarios.\n(a) The standard error of ˆp when (I) n = 125 or (II) n = 500.\n(b) The margin of error of a confidence interval when the confidence level is (I) 90% or (II) 80%.\n(c) The p-value for a Z-statistic of 2.5 calculated based on a (I) sample withn = 500 or based on a (II) sample\nwith n = 1000.\n(d) The probability of making a Type 2 Error when the alternative hypothesis is true and the significance\nlevel is (I) 0.05 or (II) 0.10.\n24Rasmussen Reports survey, Most Favor Minimum Wage of $10.50 Or Higher, April 16, 2019.", "The probability of making a type I error is the easiest to control and often also considered to\nbe the most important of the two. If we reject  whenever the p-value is less than , then the\nprobability of committing a type I error if  is true is also . For this reason,  is often called\nthe type I error rate. A common choice is to use  as the cut-off, meaning that the null\nhypothesis is falsely rejected  of all studies where it in fact was true, or that 1 study in 20\nfinds statistical evidence for alternative hypotheses that are false.\nNo, wait, let me stop myself right there, because I just lied to you and we probably shouldn’t let\nthat slide. I just claimed that if we reject  when the p-value is less than , the probability of\ncommitting a type I error is also . Indeed, you’ll see similar statements in many different\ntexts. This is an oversimplification that only is true in idealised situations that no one has ever", "that only 5% of true null hypotheses are incorrectly rejected. However, this doesn’t mean that we don’t\ncare about Type II errors. In fact, from the researcher’s perspective, the error of failing to reject the\nnull when it is actually false is an extremely annoying one. With that in mind, a secondary goal of\nhypothesis testing is to try to minimise β, the Type II error rate, although we don’t usually talk in terms\nof minimising Type II errors. Instead, we talk about maximising the power of the test. Since power is\ndeﬁned as 1 ´β, this is the same thing.\n11.8.1 The power function\nLet’s take a moment to think about what a Type II error actually is. A Type II error occurs when the\nalternative hypothesis is true, but we are nevertheless unable to reject the null hypothesis. Ideally, we’d\nbe able to calculate a single number β that tells us the Type II error rate, in the same way that we can", "(a) Write the hypotheses in words.\n(b) What is a Type 1 Error in this context?\n(c) What is a Type 2 Error in this context?\n(d) Which error is more problematic for the restaurant owner? Why?\n(e) Which error is more problematic for the diners? Why?\n(f) As a diner, would you prefer that the food safety inspector requires strong evidence or very strong\nevidence of health concerns before revoking a restaurant’s license? Explain your reasoning.\n5.30 True or false. Determine if the following statements are true or false, and explain your reasoning. If\nfalse, state how it could be corrected.\n(a) If a given value (for example, the null hypothesized value of a parameter) is within a 95% confidence\ninterval, it will also be within a 99% confidence interval.\n(b) Decreasing the significance level ( α) will increase the probability of making a Type 1 Error.\n(c) Suppose the null hypothesis is p = 0.5 and we fail to reject H0. Under this scenario, the true population\nproportion is 0.5."]}
{"query": "What is heteroscedasticity and how do you detect it?", "relevant_passages": ["the errors, or heteroscedasticity, from the presence of a funnel shape in heterosceda-\nsticitythe residual plot. An example is shown in the left-hand panel of Figure 3.11,\nin which the magnitude of the residuals tends to increase with the ﬁtted\nvalues. When faced with this problem, one possible solution is to trans-\nform the response Y using a concave function such as log Y or\n√\nY. Such\na transformation results in a greater amount of shrinkage of the larger re-\nsponses, leading to a reduction in heteroscedasticity. The right-hand panel\nof Figure 3.11 displays the residual plot after transforming the response", "We could also plot the observed values against the fitted values:\nLinear models are fitted and analysed using a number of assumptions, most of which are\nassessed by looking at plots of the model residuals, , where  is the fitted value for\nobservation . Some important assumptions are:\nThe model is linear in the parameters: we check this by looking for non-linear patterns in\nthe residuals, or in the plot of observed values against fitted values.\nThe observations are independent: which can be difficult to assess visually. We’ll look at\nmodels that are designed to handle correlated observations in Sections 8.8 and 11.6.\nHomoscedasticity: which means that the random errors all have the same variance. We\ncheck this by looking for non-constant variance in the residuals. The opposite of\nhomoscedasticity is heteroscedasticity.\nNormally distributed random errors: this assumption is important if we want to use the", "Residuals vs. fitted: look for patterns that can indicate non-linearity, e.g., that the residuals\nall are high in some areas and low in others. The blue line is there to aid the eye – it\nshould ideally be relatively close to a straight line (in this case, it isn’t perfectly straight,\nwhich could indicate a mild non-linearity).\nNormal Q-Q: see if the points follow the line, which would indicate that the residuals\n(which we for this purpose can think of as estimates of the random errors) follow a normal\ndistribution.\nScale-Location: similar to the residuals vs. fitted plot, this plot shows whether the\nresiduals are evenly spread for different values of the fitted values. Look for patterns in\nhow much the residuals vary – if, e.g., they vary more for large fitted values, then that is a\nsign of heteroscedasticity. A horizontal blue line is a sign of homoscedasticity.\nCook’s distance: look for points with high values. A commonly cited rule-of-thumb (Cook &", "homoscedasticity is heteroscedasticity.\nNormally distributed random errors: this assumption is important if we want to use the\ntraditional parametric p-values, confidence intervals, and prediction intervals. If we use\npermutation p-values or bootstrap confidence intervals (as we will later in this chapter), we\nno longer need this assumption.\nAdditionally, residual plots can be used to find influential points that (possibly) have a large\nimpact on the model coefficients (influence is measured using Cook’s distance and potential\ninfluence using leverage). We’ve already seen that we can use  plot(m)  to create some\ndiagnostic plots. To get more and better-looking plots, we can use the  autoplot  function for\n lm  objects from the  ggfortify  package:\nn <- nrow(mtcars)\nmodels <- data.frame(Observed = rep(mtcars$mpg, 2),\n                     Fitted = c(predict(m1), predict(m2)),\n                     Model = rep(c(\"Model 1\", \"Model 2\"), c(n, n)))\nggplot(models, aes(Fitted, Observed)) +", "8.8.2  Model diagnostics\nAs for any linear model, residual plots are useful for diagnostics for linear mixed models. Of\nparticular interest are signs of heteroscedasticity, as homoscedasticity is assumed in the\nmixed model. We’ll use  fortify.merMod  to turn the model into an object that can be used with\n ggplot2 , and then create some residual plots:\n05/11/25, 11:08 8 Regression models | Modern Statistics with R\nhttps://www.modernstatisticswithr.com/regression.html 73/86"]}
{"query": "How does bootstrapping work and what is it used for?", "relevant_passages": ["empirical distribution of the data. Instead of analytically deriving a formula that describes the\nstatistic’s distribution, we find a good approximation of the distribution of the statistic by using\nsimulation. We can then use that distribution to obtain confidence intervals and p-values, just\nas in the parametric case.\nThe simulation step is important. We use a process known as resampling, where we\nrepeatedly draw new observations with replacement from the original sample. We draw \nsamples this way, each with the same size  as the original sample. Each randomly drawn\nsample – called a bootstrap sample – will include different observations. Some observations\nfrom the original sample may appear more than once in a specific bootstrap sample, and some\nnot at all. For each bootstrap sample, we compute the statistic in which we are interested. This\ngives us  observations of this statistic, which together form what is called the bootstrap", "Confidence intervals and hypothesis tests are always based on a statistic, i.e., a quantity that\nwe compute from the samples. The statistic could be the sample mean, a proportion, the\nPearson correlation coefficient, or something else. In traditional parametric methods, we start\nby assuming that our data follows some distribution. For different reasons, including\nmathematical tractability, a common assumption is that the data is normally distributed. Under\nthat assumption, we can then derive the distribution of the statistic that we are interested in\nanalytically, like Gosset did for the t-test. That distribution can then be used to compute\nconfidence intervals and p-values.\nWhen using a bootstrap method, we follow the same steps, but we use the observed data and\nsimulation instead. Rather than making assumptions about the distribution , we use the\nempirical distribution of the data. Instead of analytically deriving a formula that describes the", "that the data are (for instance) normally distributed, just assume that the population looks the\nsame as your sample, and then use computers to simulate the sampling distribution for your test\nstatistic if that assumption holds. Despite relying on a somewhat dubious assumption (i.e., the\npopulation distribution is the same as the sample!) bootstrapping is quick and easy method that\nworks remarkably well in practice for lots of data analysis problems.\n• Cross validation. One question that pops up in my stats classes every now and then, usually by\na student trying to be provocative, is “Why do we care about inferential statistics at all? Why not\njust describe your sample?” The answer to the question is usually something like this: “Because\nour true interest as scientists is not the speciﬁc sample that we have observed in the past , we want\nto make predictions about data we might observe in the future ”. A lot of the issues in statistical", "5.2 The Bootstrap 187\n5.2 The Bootstrap\nThe bootstrapis a widely applicable and extremely powerful statistical toolbootstrap\nthat can be used to quantify the uncertainty associated with a given esti-\nmator or statistical learning method. As a simple example, the bootstrap\ncan be used to estimate the standard errors of the coeﬃcients from a linear\nregressionﬁt. In the speciﬁc caseof linear regression,this is not particularly\nuseful, since we saw in Chapter 3 that standard statistical software such as\nR outputs such standard errors automatically. However, the power of the\nbootstrap lies in the fact that it can be easily applied to a wide range of\nstatistical learning methods, including some for which a measure of vari-\nability is otherwise diﬃcult to obtain and is not automatically output by\nstatistical software.\nIn this section we illustrate the bootstrap on a toy example in which we\nwish to determine the best investment allocation under a simple model.", "5.2 The Bootstrap 187\n5.2 The Bootstrap\nThe bootstrapis a widely applicable and extremely powerful statistical toolbootstrap\nthat can be used to quantify the uncertainty associated with a given esti-\nmator or statistical learning method. As a simple example, the bootstrap\ncan be used to estimate the standard errors of the coeﬃcients from a linear\nregressionﬁt. In the speciﬁc caseof linear regression,this is not particularly\nuseful, since we saw in Chapter 3 that standard statistical software such as\nR outputs such standard errors automatically. However, the power of the\nbootstrap lies in the fact that it can be easily applied to a wide range of\nstatistical learning methods, including some for which a measure of vari-\nability is otherwise diﬃcult to obtain and is not automatically output by\nstatistical software.\nIn this section we illustrate the bootstrap on a toy example in which we\nwish to determine the best investment allocation under a simple model."]}
{"query": "Explain the idea behind Bayesian inference.", "relevant_passages": ["The Bayesian approach (2)\n \n◮\t Now, we still observe data, assumed to be randomly generated \nby some process. Under some assumptions (e.g., parametric \ndistribution), this process is associated with some ﬁxed object. \n◮\t We have a prior belief about it. \n◮\t Using the data, we want to update that belief and transform \nit into a posterior belief. \n3/17", "�\t � � � \nThe Bayesian approach (5) \nExample (continued) \n◮\t In our statistical experiment, X1, . . . , X n are assumed to be \ni.i.d. Bernoulli r.v. with parameter p conditionally on p. \n◮\t After observing the available sample X1, . . . , X n, we can \nupdate our belief about p by taking its distribution \nconditionally on the data. \n◮\t The distribution of p conditionally on the data is called the \nposterior distribution . \n◮\t Here, the posterior distribution is \nn n \nB a + Xi, a + n − Xi . \ni=1 i=1 \n6/17", "Pph|dq“ Ppd|hqPphq\nPpdq\nAnd this formula, folks, is known as Bayes’ rule. It describes how a learner starts out with prior beliefs\nabout the plausibility of diﬀerent hypotheses, and tells you how those beliefs should be revised in the\nface of data. In the Bayesian paradigm, all statistical inference ﬂows from this one simple rule.\n17.2\nBayesian hypothesis tests\nIn Chapter 11 I described the orthodox approach to hypothesis testing. It took an entire chapter to\ndescribe, because null hypothesis testing is a very elaborate contraption that people ﬁnd very hard to\nmake sense of. In contrast, the Bayesian approach to hypothesis testing is incredibly simple. Let’s pick\na setting that is closely analogous to the orthodox scenario. There are two hypotheses that we want to\ncompare, a null hypothesis h0 and an alternative hypothesis h1. Prior to running the experiment we have\nsome beliefs Pphqabout which hypotheses are true. We run an experiment and obtain data d. Unlike", "The Bayesian approach (3) \nExample \n◮\t Let p be the proportion of woman in the population. \n◮\t Sample n people randomly with replacement in the population \nand denote by X1, . . . , X n their gender (1 for woman, 0 \notherwise). \n◮\t In the frequentist approach, we estimated p (using the MLE), \nwe constructed some conﬁdence interval for p, we did \nhypothesis testing (e.g., H0 : p = .5 v.s. H1 : p  = .5). \n◮\t Before analyzing the data, we may believe that p is likely to \nbe close to 1/2. \n◮\t The Bayesian approach is a tool to: \n1.\t include mathematically our prior belief in statistical procedures. \n2.\t update our prior belief using the data. \n4/17", "The Bayesian approach (4) \nExample (continued) \n◮ Our prior belief about p can be quantiﬁed: \n◮ E.g., we are 90% sure that p is between .4 and .6, 95% that it \nis between .3 and .8, etc... \n◮ Hence, we can model our prior belief using a distribution for \np, as if p was random. \n◮ In reality, the true parameter is not random ! However, the \nBayesian approach is a way of modeling our belief about the \nparameter by doing as if it was random. \n◮ E.g., p ∼ B (a, a) (Beta distribution ) for some a > 0. \n◮ This distribution is called the prior distribution . \n5/17"]}
{"query": "What is the purpose of the likelihood function?", "relevant_passages": ["distribution of y(i) as y(i) |x(i); θ∼N(θTx(i),σ2).\nGiven X (the design matrix, which contains all the x(i)’s) and θ, what\nis the distribution of the y(i)’s? The probability of the data is given by\np(⃗ y|X; θ). This quantity is typically viewed a function of ⃗ y(and perhaps X),\nfor a ﬁxed value of θ. When we wish to explicitly view this as a function of\nθ, we will instead call it the likelihood function:\nL(θ) = L(θ; X,⃗ y) = p(⃗ y|X; θ).\nNote that by the independence assumption on the ϵ(i)’s (and hence also the\ny(i)’s given the x(i)’s), this can also be written\nL(θ) =\nn∏\ni=1\np(y(i) |x(i); θ)\n=\nn∏\ni=1\n1√\n2πσ exp\n(\n−(y(i) −θTx(i))2\n2σ2\n)\n.\nNow, given this probabilistic model relating the y(i)’s and the x(i)’s, what\nis a reasonable way of choosing our best guess of the parameters θ? The\nprincipal of maximum likelihood says that we should choose θ so as to\nmake the data as high probability as possible. I.e., we should choose θ to\nmaximize L(θ).", "�\nLikelihood, Continuous case (1)\n \nLet \n(\nE, (I Pθ)θ∈Θ\n) \nbe a statistical model associated with a sample \nof i.i.d. r.v. X1, . . . , X n. Assume that all the I Pθ have density fθ. \nDeﬁnition \nThe likelihood of the model is the map L deﬁned as: \nL : En × Θ → I R \nn(x1, . . . , x n, θ)  → fθ(xi).i=1 \n18/23", "mathematical equation called alikelihood function: likelihood\nfunctionℓ(β0,β1)=\n∏\ni:yi=1\np(xi)\n∏\ni′:yi′=0\n(1−p(xi′)). (4.5)\nThe estimates ˆβ0 and ˆβ1 are chosen tomaximize this likelihood function.\nMaximum likelihood is a very general approach that is used to ﬁt many\nof the non-linear models that we examine throughout this book. In the\nlinear regression setting, the least squares approach is in fact a special case\nof maximum likelihood. The mathematical details of maximum likelihood\nare beyond the scope of this book. However, in general, logistic regression\nand other models can be easily ﬁt using a statistical software package such\nas R, and so we do not need to concern ourselves with the details of the\nmaximum likelihood ﬁtting procedure.\nTable 4.1 shows the coeﬃcient estimates and related information that\nresult from ﬁtting a logistic regression model on theDefault data in order\nto predict the probability ofdefault=Yes using balance.W es e et h a tˆβ1 =", "mathematical equation called alikelihood function: likelihood\nfunctionℓ(β0,β1)=\n∏\ni:yi=1\np(xi)\n∏\ni′:yi′=0\n(1−p(xi′)). (4.5)\nThe estimates ˆβ0 and ˆβ1 are chosen tomaximize this likelihood function.\nMaximum likelihood is a very general approach that is used to ﬁt many\nof the non-linear models that we examine throughout this book. In the\nlinear regression setting, the least squares approach is in fact a special case\nof maximum likelihood. The mathematical details of maximum likelihood\nare beyond the scope of this book. However, in general, logistic regression\nand other models can be easily ﬁt using a statistical software package such\nas R, and so we do not need to concern ourselves with the details of the\nmaximum likelihood ﬁtting procedure.\nTable 4.1 shows the coeﬃcient estimates and related information that\nresult from ﬁtting a logistic regression model on theDefault data in order\nto predict the probability ofdefault=Yes using balance.W es e et h a tˆβ1 ="]}
{"query": "Describe how to test whether two distributions differ significantly.", "relevant_passages": ["populations.\nThe Welch t-test: where the variance is allowed to differ between the two populations.\nThe equal-variances t-test will not work properly when the two populations have differing\nvariances – a low p-value may be due to differences in variances rather than differences in\nmeans. In such cases, it no longer tests the hypotheses we were interested in; instead, it tests\nwhether the distribution is the same for the two populations. If that is the question that you’re\ninterested in, there are other, better, tests that you can use instead, such as the Kolmogorov-\nSmirnov test, implemented in  ks.test .\nThe Welch t-test works well both when the population variances differ and when they are\nequal, and it tests the right hypotheses in both cases. It is the default in R, and I strongly\nrecommend using it instead of the equal-variances test. Some textbooks recommend first\nperforming a test to see whether the population variances differ, and then using that to choose", "Chapter 8: Probability distributions 37\nMethod B: 80.02 79.94 79.98 79.97 79.97 80.03 79.95 79.97\nBoxplots provide a simple graphical comparison of the two samples.\nA <- scan()\n79.98 80.04 80.02 80.04 80.03 80.03 80.04 79.97\n80.05 80.03 80.02 80.00 80.02\nB <- scan()\n80.02 79.94 79.98 79.97 79.97 80.03 79.95 79.97\nboxplot(A, B)\nwhich indicates that the first group tends to give higher results than the second.\n1 2\n79.94 79.96 79.98 80.00 80.02 80.04\nTo test for the equality of the means of the two examples, we can use an unpaired t-test by\n> t.test(A, B)\nWelch Two Sample t-test\ndata: A and B\nt = 3.2499, df = 12.027, p-value = 0.00694\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n0.01385526 0.07018320\nsample estimates:\nmean of x mean of y\n80.02077 79.97875\nwhich does indicate a significant difference, assuming normality. By default the R function does", "mean of x mean of y\n80.02077 79.97875\nwhich does indicate a significant difference, assuming normality. By default the R function does\nnot assume equality of variances in the two samples. We can use the F test to test for equality\nin the variances, provided that the two samples are from normal populations.\n> var.test(A, B)\nF test to compare two variances\ndata: A and B\nF = 0.5837, num df = 12, denom df = 7, p-value = 0.3938\nalternative hypothesis: true ratio of variances is not equal to 1"]}
{"query": "What is multicollinearity and how can it impact regression?", "relevant_passages": ["To avoid such a situation, it is desirable to identify and address potential\ncollinearity problems while ﬁtting the model.\nA simple way to detect collinearity is to look at the correlation matrix\nof the predictors. An element of this matrix that is large in absolute value\nindicates a pair of highly correlated variables, and therefore a collinearity\nproblem in the data. Unfortunately, not all collinearity problems can be\ndetected by inspection of the correlation matrix: it is possible for collinear-\nity to exist between three or more variables even if no pair of variables\nhas a particularly high correlation. We call this situationmulticollinearity. multi-\ncollinearityInstead of inspecting the correlation matrix, a better way to assess multi-\ncollinearity is to compute thevariance inﬂation factor (VIF). The VIF isvariance\ninﬂation\nfactor\nthe ratio of the variance ofˆβj when ﬁtting the full model divided by the\nvariance of ˆβj if ﬁt on its own. The smallest possible value for VIF is 1,", "In this case, there are some highly correlated pairs,  hp  and  disp  among them. As a\nnumerical measure of collinearity, we can use the generalised variance inflation factor (GVIF),\ngiven by the  vif  function in the  car  package:\nA high GVIF indicates that a variable is highly correlated with other explanatory variables in\nthe dataset. Recommendations for what a “high GVIF” is varies, from 2.5 to 10 or more.\nYou can mitigate problems related to multicollinearity by:\nRemoving one or more of the correlated variables from the model (because they are\nstrongly correlated, they measure almost the same thing anyway!),\nCentring your explanatory variables (particularly if you include polynomial terms) – more\non this in the next section,\nUsing a regularised regression model (which we’ll do in Section 11.4).\nExercise 8.4 Below are two simulated datasets. One exhibits a non-linear dependence\nbetween the variables, and the other exhibits heteroscedasticity. Fit a model with  y  as the", "dures in the high-dimensional setting, we must be quite cautious in the way\nthat we report the results obtained. In Chapter 3, we learned aboutmulti-\ncollinearity, the concept that the variables in a regression might be corre-\nlated with each other. In the high-dimensional setting, the multicollinearity\nproblem is extreme: any variable in the model can be written as a linear\ncombination of all of the other variables in the model. Essentially, this\nmeans that we can never know exactly which variables (if any) truly are\npredictive of the outcome, and we can never identify thebest coeﬃcients\nfor use in the regression. At most, we can hope to assign large regression\ncoeﬃcients to variables that are correlated with the variables that truly are\npredictive of the outcome.\nFor instance, suppose that we are trying to predict blood pressure on the\nbasis of half a million SNPs, and that forward stepwise selection indicates", "on the least squares ﬁt in Figure 3.12: it has low leverage.\n6. Collinearity\nCollinearity refers to the situation in which two or more predictor variablescollinearity\nare closely related to one another. The concept of collinearity is illustrated\nin Figure 3.14 using the Credit data set. In the left-hand panel of Fig-\nure 3.14, the two predictorslimit and age appear to have no obvious rela-\ntionship. In contrast, in the right-hand panel of Figure 3.14, the predictors\nlimit and rating are very highly correlated with each other, and we say\nthat they arecollinear. The presence of collinearity can pose problems in\nthe regression context, since it can be diﬃcult to separate out the indi-\nvidual eﬀects of collinear variables on the response. In other words, since\nlimit and rating tend to increase or decrease together, it can be diﬃcult to\ndetermine howeachone separatelyis associatedwith the response,balance.\nFigure3.15illustratessomeofthediﬃcultiesthatcanresultfromcollinear-", "variable provides about the response is redundant in the presence of the\nother variables. For instance, if we regressbalance onto age and limit,\nwithout the rating predictor, then the resulting VIF values are close to\nthe minimum possible value of 1, and theR2 drops from 0.754 to 0.75.\nSo dropping rating from the set of predictors has eﬀectively solved the\ncollinearity problem without compromising the ﬁt. The second solution is\nto combine the collinear variables together into a single predictor. For in-\nstance, we might take the average of standardized versions oflimit and\nrating in order to create a new variable that measurescredit worthiness.\n3.4 The Marketing Plan\nWe now brieﬂy return to the seven questions about theAdvertising data\nthat we set out to answer at the beginning of this chapter.\n1. Is there a relationship between advertising sales and budget?\nThis question can be answered by ﬁtting a multiple regression model"]}
{"query": "When is a chi-square test appropriate?", "relevant_passages": ["against the null hypothesis.\nCONDITIONS FOR THE CHI-SQUARE TEST\nThere are two conditions that must be checked before performing a chi-square test:\nIndependence. Each case that contributes a count to the table must be independent of all\nthe other cases in the table.\nSample size / distribution. Each particular scenario (i.e. cell count) must have at least\n5 expected cases.\nFailing to check conditions may affect the test’s error rates.\nWhen examining a table with just two bins, pick a single bin and use the one-proportion\nmethods introduced in Section 6.1.", "12.5\nAssumptions of the test(s)\nAll statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are\nmet. For the chi-square tests discussed so far in this chapter, the assumptions are:\n• Expected frequencies are suﬃciently large. Remember how in the previous section we saw that the\nχ2 sampling distribution emerges because the binomial distribution is pretty similar to a normal\ndistribution? Well, like we discussed in Chapter 9 this is only true when the number of observations\nis suﬃciently large. What that means in practice is that all of the expected frequencies need to be\nreasonably big. How big is reasonably big? Opinions diﬀer, but the default assumption seems to be\nthat you generally would like to see all your expected frequencies larger than about 5, though for\nlarger tables you would probably be okay if at least 80% of the the expected frequencies are above 5", "larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5\nand none of them are below 1. However, from what I’ve been able to discover (e.g., Cochran, 1954),\nthese seem to have been proposed as rough guidelines, not hard and fast rules; and they seem to\nbe somewhat conservative (Larntz, 1978).\n• Data are independent of one another . One somewhat hidden assumption of the chi-square test is\nthat you have to genuinely believe that the observations are independent. Here’s what I mean.\nSuppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk\naround the maternity wards, and observe 20 girls and only 10 boys. Seems like a pretty convincing\ndiﬀerence, right? But later on, it turns out that I’d actually walked into the same ward 10 times,\nand in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations", "highly non-random behaviour from people, but in this case, I would get an observed frequency of 50\nfour all four suits. For this example, the fact that the observations are non-independent (because\nthe four cards that you pick will be related to each other) actually leads to the opposite eﬀect...\nfalsely retaining the null.\nIf you happen to ﬁnd yourself in a situation where independence is violated, it may be possible to use the\nMcNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected\ncell counts are too small, check out the Fisher exact test. It is to these topics that we now turn.\n12.6\nThe most typical way to do chi-square tests in R\nWhen discussing how to do a chi-square goodness of ﬁt test (Section 12.1.7) and the chi-square test\nof independence (Section 12.2.2), I introduced you to two separate functions in the lsr package. We\nran our goodness of ﬁt tests using the goodnessOfFitTest() function, and our tests of independence"]}
{"query": "What does it mean for an estimator to be unbiased?", "relevant_passages": ["squares line.\nThe analogy between linear regression and estimation of the mean of a\nrandom variable is an apt one based on the concept ofbias.I fw eu s et h ebias\nsample mean ˆμ to estimateμ, this estimate isunbiased, in the sense thatunbiased\non average,we expect ˆμ to equalμ. What exactly does this mean? It means\nthat on the basis of one particular set of observationsy1,...,y n,ˆμ might\noverestimate μ, and on the basis of another set of observations, ˆμ might\nunderestimate μ. But if we could average a huge number of estimates of\nμ obtained from a huge number of sets of observations, then this average\nwouldexactly equalμ.Hence,anunbiasedestimatordoesnot systematically\nover- or under-estimate the true parameter. The property of unbiasedness\nholds for the least squares coeﬃcient estimates given by (3.4) as well: if\nwe estimate β0 and β1 on the basis of a particular data set, then our\nestimates won’t be exactly equal toβ0 and β1. But if we could average", "The average of these hypothetical medians is also very close to the actual population mean.\nThese results demonstrate that the sample mean and median are unbiased estimators,\nwhich means that they are correct on average. The word “bias” means different things in\ndifferent contexts, which can be a source of confusion. In this context, “unbiased” means that\nthe average of the estimates is the actual value.\nSo far, we’ve shown that both estimators are consistent and unbiased, but it’s still not clear\nwhich is better. Let’s try one more experiment: let’s see which estimator is more accurate.\nThe word “accurate” also means different things in different contexts – as one way to\nquantify it, let’s consider the mean squared error (MSE). The following function computes\nthe differences between the estimates and the actual value, and returns the mean of the\nsquares of these errors.\nNotice that we can only compute MSE if we know the actual value. In practice, we usually", "8.8. Glossary\npopulation mean: The true mean of a quantity in an entire population, as opposed to\nthe sample mean, which is calculated from a subset.\nparameter: One of the values that specify a particular distribution in a set of\ndistributions – for example, the parameters of a normal distribution are the mean and\nstandard deviation.\nestimator: A statistic calculated from a sample that is used to estimate a parameter of\nthe population.\nconsistent: An estimator is consistent if it converges to the actual value of a parameter\nas the sample size increases.\nunbiased: An estimator is unbiased if, for a particular sample size, the average of the\nsample estimates is the actual value of the parameter.\nmean squared error (MSE): A measure of the accuracy of an estimator – it’s the\naverage squared difference between estimated and true parameter values, assuming the\ntrue value is known.\nrobust: An estimator is robust if it remains accurate even when a dataset contains", "As you can see, the estimates given by the different approaches differ, so clearly the choice of\nestimator matters. We can’t determine which to use based on a single sample though. Instead,\nwe typically compare the long-run properties of estimators, such as their bias and variance.\nThe bias is the difference between the mean of the estimator and the parameter it seeks to\nestimate. An estimator is unbiased if its bias is 0, which is considered desirable at least in this\nsetting. Among unbiased estimators, we prefer the one that has the smallest variance. So how\ncan we use simulation to compute the bias and variance of estimators?\nThe key to using simulation here is to realise that  x_mean  is an observation of the random\nvariable  where each  is -distributed. We can\ngenerate observations of  (using  rnorm ), and can therefore also generate observations of\n. That means that we can obtain an arbitrarily large sample of observations of , which we", "� � \n� � \nParameter estimation (2)\n \n◮ Bias of an estimator θˆn of θ: \nˆI E θn − θ. \n◮ Risk (or quadratic risk ) of an estimator θˆn: \nI E |θˆn − θ|2 . \nRemark: If Θ ⊆ I R, \n”Quadratic risk = bias 2 + variance”. \n8/11"]}
{"query": "How do you interpret the R² and adjusted R² metrics?", "relevant_passages": ["70 3. Linear Regression\nTo calculateR2,w eu s et h ef o r m u l a\nR2 = TSS−RSS\nTSS =1 − RSS\nTSS (3.17)\nwhere TSS =∑ (yi − ¯y)2 is the total sum of squares, and RSS is deﬁnedtotal sum of\nsquaresin (3.16). TSS measures the total variance in the responseY,a n dc a nb e\nthought of as the amount of variability inherent in the response before the\nregressionisperformed.Incontrast,RSSmeasurestheamountofvariability\nthat is left unexplained after performing the regression. Hence, TSS−RSS\nmeasures the amount of variability in the response that is explained (or\nremoved) by performing the regression, andR2 measures the proportion\nof variability inY that can be explained usingX.A n R2 statistic that is\nclose to 1 indicates that a large proportion of the variability in the response\nhas been explained by the regression. A number near 0 indicates that the\nregressiondid not explainmuchofthe variabilityin the response;this might\noccur because the linear model is wrong, or the inherent errorσ2 is high,", "expect R2 “1. Similarly, if the model is completely useless, we would like R2 to be equal to 0. What\ndo I mean by “useless”? Tempting as it is demand that the regression model move out of the house, cut\nits hair and get a real job, I’m probably going to have to pick a more practical deﬁnition: in this case,\nall I mean is that the residual sum of squares is no smaller than the total sum of squares, SS res “SStot.\nWait, why don’t we do exactly that? The formula that provides us with out R2 value is pretty simple to\nwrite down,\nR2 “1 ´SSres\nSStot\nand equally simple to calculate in R:\n> R.squared <- 1 - (SS.resid / SS.tot)\n> print( R.squared )\n[1] 0.8161018\nThe R2 value, sometimes called the coeﬃcient of determination 3 has a simple interpretation: it is\nthe proportion of the variance in the outcome variable that can be accounted for by the predictor. So in\nthis case, the fact that we have obtained R2 “.816 means that the predictor ( my.sleep) explains 81.6%", "regressiondid not explainmuchofthe variabilityin the response;this might\noccur because the linear model is wrong, or the inherent errorσ2 is high,\nor both. In Table 3.2, theR2 was 0.61, and so just under two-thirds of the\nvariability insales is explained by a linear regression onTV.\nThe R2 statistic (3.17) has an interpretational advantage over the RSE\n(3.15), since unlike the RSE, it always lies between 0 and 1. However, it can\nstill be challenging to determine what is agood R2 value, and in general,\nthis will depend on the application. For instance, in certain problems in\nphysics, we may know that the data truly comes from a linear model with\na small residual error. In this case, we would expect to see anR2 value that\nisextremelycloseto1,andasubstantiallysmaller R2 valuemightindicatea\nserious problem with the experiment in which the data were generated. On\nthe other hand, in typical applications in biology, psychology, marketing,", "15.4.3 The adjusted R2 value\nOne ﬁnal thing to point out before moving on. It’s quite common for people to report a slightly\ndiﬀerent measure of model performance, known as “adjusted R2”. The motivation behind calculating\nthe adjusted R2 value is the observation that adding more predictors into the model will always call\nthe R2 value to increase (or at least not decrease). The adjusted R2 value introduces a slight change\nto the calculation, as follows. For a regression model with K predictors, ﬁt to a data set containing N\nobservations, the adjusted R2 is:\nadj. R2 “1 ´\nˆSSres\nSStot\nˆ N ´1\nN ´K´1\n˙\nThis adjustment is an attempt to take the degrees of freedom into account. The big advantage of the\nadjusted R2 value is that when you add more predictors to the model, the adjusted R2 value will only\nincrease if the new variables improve the model performance more than you’d expect by chance. The big", "it becomes less helpful when there are many variables. The regular R2 is a biased estimate of the\namount of variability explained by the model when applied to a new sample of data. To get a better\nestimate, we use the adjusted R2.\nADJUSTED R2R2R2 AS A TOOL FOR MODEL ASSESSMENT\nThe adjusted R2R2R2 is computed as\nR2\nadj = 1 − s2\nresiduals/(n − k − 1)\ns2\noutcome/(n − 1) = 1 − s2\nresiduals\ns2\noutcome\n× n − 1\nn − k − 1\nwhere n is the number of cases used to fit the model and k is the number of predictor variables\nin the model. Remember that a categorical predictor with p levels will contribute p − 1 to the\nnumber of variables in the model.\nBecause k is never negative, the adjusted R2 will be smaller – often times just a little smaller\n– than the unadjusted R2. The reasoning behind the adjusted R2 lies in the degrees of freedom\nassociated with each variance, which is equal to n − k − 1 for the multiple regression context. If we"]}
{"query": "What is survival analysis and when is it used?", "relevant_passages": ["Survival analysis\nContents\n13.1. Survival Functions\n13.2. Hazard Function\n13.3. Marriage Data\n13.4. Weighted Bootstrap\n13.5. Estimating Hazard Functions\n13.6. Estimating Survival Functions\n13.7. Lifelines\n13.8. Confidence Intervals\n13.9. Expected Remaining Lifetime\n13.10. Glossary\n13.11. Exercises\nThe third edition of Think Stats is available now from Bookshop.org and Amazon (those are\naffiliate links). If you are enjoying the free, online version, consider buying me a coffee.\nSurvival analysis is a way to describe how long things last. It is often used to study human\nlifetimes, but it also applies to “survival” of mechanical and electronic components, or more\ngenerally to an interval in time before any kind of event – or even an interval in space.\nWe’ll start with a simple example, the lifespans of light bulbs, and then consider a more\nsubstantial example, age at first marriage and how it has changed in the United States over\nthe last 50 years.", "the problem that survival analysis solves. It is speciﬁcally designed to handle this situation, where\nyou’re systematically missing one “side” of the data because the study ended. It’s very widely used\nin health research, and in that context it is often literally used to analyse survival. For instance,\nyou may be tracking people with a particular type of cancer, some who have received treatment A\nand others who have received treatment B, but you only have funding to track them for 5 years. At\nthe end of the study period some people are alive, others are not. In this context, survival analysis\nis useful for determining which treatment is more eﬀective, and telling you about the risk of death\nthat people face over time.\n• Repeated measures ANOVA. When talking about reshaping data in Chapter 7, I introduced\nsome data sets in which each participant was measured in multiple conditions (e.g., in the drugs", "9  Survival analysis and censored data\nSurvival analysis, or time-to-event analysis, often involves censored data. Censoring also\noccurs in measurements with detection limits, often found in biomarker data and\nenvironmental data. This chapter is concerned with methods for analysing such data.\nAfter reading this chapter, you will be able to use R to:\nVisualise survival data,\nFit survival analysis models, and\nAnalyse data with left-censored observations.\n9.1  The basics of survival analysis\nMany studies are concerned with the time until an event happens: time until a machine fails,\ntime until a patient diagnosed with a disease dies, and so on. In this section we will consider\nsome methods for survival analysis (also known as reliability analysis in engineering and\nduration analysis in economics), which is used for analysing such data. The main difficulty\nhere is that studies often end before all participants have had events, meaning that some", "here is that studies often end before all participants have had events, meaning that some\nobservations are right-censored – for these observations, we don’t know when the event\nhappened, but only that it happened after the end of the study.\nThe  survival  package contains a number of useful methods for survival analysis. Let’s install\nit:\nWe will study the lung cancer data in  lung :\ninstall.packages(\"survival\")\nlibrary(survival)\n?lung\nView(lung)\n05/11/25, 11:09 9 Survival analysis and censored data | Modern Statistics with R\nhttps://www.modernstatisticswithr.com/survivalchapter.html 1/29", "Between weeks 36 and 39, the expected remaining time decreases until, at the beginning of\nthe 39th week, it is about 0.6 weeks. But after that, the curve levels off. At the beginning of\nthe 40th week, the expected remaining time is still close to 0.6 weeks – actually a little higher\n– and at the beginning of the 41st, 42nd, and 43rd, it is almost the same. For people waiting\nanxiously for a baby to be born, this behavior seems quite cruel.\n13.10. Glossary\nsurvival analysis: A set of methods for describing and predicting the time until an event\nof interest, often focused on lifetimes or durations.\nsurvival function: A function that maps from a time, , to the probability of surviving\nbeyond .\nhazard function: A function that maps from  to the fraction of cases that experience\nthe event at , out of all cases that survive until  .\n \nfor i in range(21):\n    sample = resample_rows_weighted(live, \"finalwgt\")\n    pmf_durations = Pmf.from_seq(sample[\"prglngth\"])"]}
{"query": "Explain the difference between MAP and MLE estimation.", "relevant_passages": ["because it requires taking integrals over the (usually high-dimensional) θ as\nin Equation (9.3), and this typically cannot be done in closed-form.\nThus, in practice we will instead approximate the posterior distribution\nfor θ. One common approximation is to replace our posterior distribution for\nθ (as in Equation 9.4) with a single point estimate. The MAP (maximum\na posteriori) estimate for θ is given by\nθMAP = arg max\nθ\nn∏\ni=1\np(y(i)|x(i),θ)p(θ). (9.5)\nNote that this is the same formulas as for the MLE (maximum likelihood)\nestimate for θ, except for the prior p(θ) term at the end.\nIn practical applications, a common choice for the prior p(θ) is to assume\nthat θ∼N(0,τ2I). Using this choice of prior, the ﬁtted parametersθMAP will\nhave smaller norm than that selected by maximum likelihood. In practice,\nthis causes the Bayesian MAP estimate to be less susceptible to overﬁtting\nthan the ML estimate of the parameters. For example, Bayesian logistic", "this causes the Bayesian MAP estimate to be less susceptible to overﬁtting\nthan the ML estimate of the parameters. For example, Bayesian logistic\nregression turns out to be an eﬀective algorithm for text classiﬁcation, even\nthough in text classiﬁcation we usually have d≫n.\n7Since we are now viewing θ as a random variable, it is okay to condition on it value,\nand write “p(y|x,θ)” instead of “ p(y|x; θ).”\n8The integral below would be replaced by a summation if y is discrete-valued.", "Maximum likelihood estimator (1)\n \nLet X1, . . . , X n be an i.i.d. sample associated with a statistical \nmodel \n(\nE, (I Pθ)θ∈Θ\n) \nand let L be the corresponding likelihood. \nDeﬁnition \nThe likelihood estimator of θ is deﬁned as: \nθˆMLE = argmax L(X1, . . . , X n, θ),n \nθ∈Θ \nprovided it exists. \nRemark (log-likelihood estimator): In practice, we use the fact \nthat \nθˆMLE = argmax log L(X1, . . . , X n, θ).n \nθ∈Θ \n20/23"]}
{"query": "How do you detect outliers statistically?", "relevant_passages": ["4.5  Outliers and missing data\n4.5.1  Detecting outliers\nBoth boxplots and scatterplots are helpful in detecting deviating observations – often called\noutliers. Outliers can be caused by measurement errors or errors in the data input but can also\nbe interesting rare cases that can provide valuable insights about the process that generated\nthe data. Either way, it is often of interest to detect outliers, for instance because that may\ninfluence the choice of what statistical tests to use.\nLet’s draw a scatterplot of diamond carats versus prices:\nThere are some outliers which we may want to study further. For instance, there is a\nsurprisingly cheap 5-carat diamond, and some cheap 3-carat diamonds. Note that it is not just\nthe prices nor just the carats of these diamonds that make them outliers, but the unusual\ncombinations of prices and carats. But how can we identify those points?\nOne option is to use the  plotly  package to make an interactive version of the plot, where we", "sion, there are three conceptually distinct ways in which an observation might be called “anomalous”.\nAll three are interesting, but they have rather diﬀerent implications for your analysis.\nThe ﬁrst kind of unusual observation is an outlier. The deﬁnition of an outlier (in this context) is\nan observation that is very diﬀerent from what the regression model predicts. An example is shown in\nFigure 15.5. In practice, we operationalise this concept by saying that an outlier is an observation that\n- 477 -", "328 CHAPTER 8. INTRODUCTION TO LINEAR REGRESSION\n8.3 Types of outliers in linear regression\nIn this section, we identify criteria for determining which outliers are important and influential.\nOutliers in regression are observations that fall far from the cloud of points. These points are\nespecially important because they can have a strong influence on the least squares line.\nEXAMPLE 8.17\nThere are six plots shown in Figure 8.18 along with the least squares line and residual plots. For each\nscatterplot and residual plot pair, identify the outliers and note how they influence the least squares\nline. Recall that an outlier is any point that doesn’t appear to belong with the vast majority of the\nother points.\n(1) There is one outlier far from the other points, though it only appears to slightly influence\nthe line.\n(2) There is one outlier on the right, though it is quite close to the least squares line, which\nsuggests it wasn’t very influential.", "The skewness of the bogus dataset is off by a factor of almost 40, and it has the wrong sign!\nWith the outliers added to the data, the distribution is strongly skewed to the right, as\nindicated by large positive skewness. But the distribution of the valid data is slightly skewed\nto the left, as indicated by small negative skewness.\nThese results show that a small number of outliers have a moderate effect on the mean, a\nstrong effect on the standard deviation, and a disastrous effect on skewness.\nAn alternative is to use statistics based on percentiles. Specifically:\nThe median, which is the 50th percentile, identifies a central point in a distribution, like\nthe mean.\nThe interquartile range, which is the difference between the 25th and 75th percentiles,\nquantifies the spread of the distribution, like the standard deviation.\nThe quartile skewness uses the quartiles of the distribution (25th, 50th, and 75th\npercentiles) to quantify the skewness."]}
{"query": "How do you vectorize operations in NumPy and why is it beneficial?", "relevant_passages": ["NumPyUserGuide,Release2.3.0\nufunc\nNumPy’s fast element-by-element computation (vectorization) gives a choice which function gets applied. The\ngeneral term for the function isufunc, short foruniversal function . NumPy routines have built-in\nufuncs,butuserscanalsowritetheirown.\nvectorization\nNumPy hands off array processing to C, where looping and computation are much faster than in Python. To\nexploitthis,programmersusingNumPyeliminatePythonloopsinfavorofarray-to-arrayoperations. vectorization\ncanreferbothtotheCoffloadingandtostructuringNumPycodetoleverageit.\nview\nWithouttouchingunderlyingdata,NumPycanmakeonearrayappeartochangeitsdatatypeandshape.\nAnarraycreatedthiswayisa view,andNumPyoftenexploitstheperformancegainofusingaviewversusmaking\nanewarray.\nA potential drawback is that writing to a view can alter the original as well. If this is a problem, NumPy instead\nneedstocreateaphysicallydistinctarray–a copy.", "NumPyUserGuide,Release2.3.0\nfor (i = 0; i < rows; i++) {\nfor (j = 0; j < columns; j++) {\nc[i][j] = a[i][j]*b[i][j];\n}\n}\nNumPy gives us the best of both worlds: element-by-element operations are the “default mode” when anndarray is\ninvolved,buttheelement-by-elementoperationisspeedilyexecutedbypre-compiledCcode. InNumPy\nc = a * b\ndoes what the earlier examples do, at near-C speeds, but with the code simplicity we expect from something based on\nPython. Indeed,theNumPyidiomisevensimpler! ThislastexampleillustratestwoofNumPy’sfeatureswhicharethe\nbasisofmuchofitspower: vectorizationandbroadcasting.\n1.1 WhyisNumPyfast?\nVectorizationdescribestheabsenceofanyexplicitlooping,indexing,etc.,inthecode-thesethingsaretakingplace,of\ncourse,just“behindthescenes”inoptimized,pre-compiledCcode. Vectorizedcodehasmanyadvantages,amongwhich\nare:\n• vectorizedcodeismoreconciseandeasiertoread\n• fewerlinesofcodegenerallymeansfewerbugs", "are:\n• vectorizedcodeismoreconciseandeasiertoread\n• fewerlinesofcodegenerallymeansfewerbugs\n• the code more closely resembles standard mathematical notation (making it easier, typically, to correctly code\nmathematicalconstructs)\n• vectorization results in more “Pythonic” code. Without vectorization, our code would be littered with inefficient\nanddifficulttoread for loops.\nBroadcasting is the term used to describe the implicit element-by-element behavior of operations; generally speaking,\nin NumPy all operations, not just arithmetic operations, but logical, bit-wise, functional, etc., behave in this implicit\nelement-by-element fashion, i.e., they broadcast. Moreover, in the example above,a and b could be multidimensional\narraysofthesameshape,orascalarandanarray,oreventwoarrayswithdifferentshapes,providedthatthesmallerarray\nis“expandable”totheshapeofthelargerinsuchawaythattheresultingbroadcastisunambiguous. Fordetailed“rules”\nofbroadcastingsee Broadcasting.\n1.2 WhoelseusesNumPy?", "88\nwith more matrix and vector notations. Another important motivation of\nvectorization is the speed perspective in the implementation. In order to\nimplement a neural network eﬃciently, one must be careful when using for\nloops. The most natural way to implement equation (7.15) in code is perhaps\nto use a for loop. In practice, the dimensionalities of the inputs and hidden\nunits are high. As a result, code will run very slowly if you use for loops.\nLeveraging the parallelism in GPUs is/was crucial for the progress of deep\nlearning.\nThis gave rise to vectorization. Instead of using for loops, vectorization\ntakes advantage of matrix algebra and highly optimized numerical linear\nalgebra packages (e.g., BLAS) to make neural network computations run\nquickly. Before the deep learning era, a for loop may have been suﬃcient\non smaller datasets, but modern deep networks and state-of-the-art datasets\nwill be infeasible to run with for loops."]}
{"query": "Write Python code to remove rows with missing values from a DataFrame.", "relevant_passages": ["Cumulative methods like cumsum() and cumprod() ignore NA values by default preserve them in the result. This behavior can be changed with skipna\n\nCumulative methods like cumsum() and cumprod() ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False.\nser = pd.Series([1, np.nan, 3, np.nan])\n\nser\nOut[80]: \n0    1.0\n1    NaN\n2    3.0\n3    NaN\ndtype: float64\n\nser.cumsum()\nOut[81]: \n0    1.0\n1    NaN\n2    4.0\n3    NaN\ndtype: float64\n\nser.cumsum(skipna=False)\nOut[82]: \n0    1.0\n1    NaN\n2    NaN\n3    NaN\ndtype: float64\n\nDropping missing data\n\ndropna() dropa rows or columns with missing data.\n\ndf = pd.DataFrame([[np.nan, 1, 2], [1, 2, np.nan], [1, 2, 3]])\n\ndf\nOut[84]: \n     0  1    2\n0  NaN  1  2.0\n1  1.0  2  NaN\n2  1.0  2  3.0\n\ndf.dropna()\nOut[85]: \n     0  1    2\n2  1.0  2  3.0\n\ndf.dropna(axis=1)\nOut[86]: \n   1\n0  1\n1  2\n2  2\n\nser = pd.Series([1, pd.NA], dtype=\"int64[pyarrow]\")", "df.dropna(axis=1)\nOut[86]: \n   1\n0  1\n1  2\n2  2\n\nser = pd.Series([1, pd.NA], dtype=\"int64[pyarrow]\")\n\nser.dropna()\nOut[88]: \n0    1\ndtype: int64[pyarrow]\n\nFilling missing data\n\nFilling by value\n\nfillna() replaces NA values with non-NA data.\n\nReplace NA with a scalar value\n\ndata = {\"np\": [1.0, np.nan, np.nan, 2], \"arrow\": pd.array([1.0, pd.NA, pd.NA, 2], dtype=\"float64[pyarrow]\")}\n\ndf = pd.DataFrame(data)\n\ndf\nOut[91]: \n    np  arrow\n0  1.0    1.0\n1  NaN   <NA>\n2  NaN   <NA>\n3  2.0    2.0\n\ndf.fillna(0)\nOut[92]: \n    np  arrow\n0  1.0    1.0\n1  0.0    0.0\n2  0.0    0.0\n3  2.0    2.0\n\nFill gaps forward or backward\n\ndf.ffill()\nOut[93]: \n    np  arrow\n0  1.0    1.0\n1  1.0    1.0\n2  1.0    1.0\n3  2.0    2.0\n\ndf.bfill()\nOut[94]: \n    np  arrow\n0  1.0    1.0\n1  2.0    2.0\n2  2.0    2.0\n3  2.0    2.0\n\nLimit the number of NA values filled\n\ndf.ffill(limit=1)\nOut[95]: \n    np  arrow\n0  1.0    1.0\n1  1.0    1.0\n2  NaN   <NA>\n3  2.0    2.0", "df_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n\ndf_dropna = pd.DataFrame(df_list, columns=[\"a\", \"b\", \"c\"])\n\ndf_dropna\nOut[30]: \n   a    b  c\n0  1  2.0  3\n1  1  NaN  4\n2  2  1.0  3\n3  1  2.0  2\n\n# Default ``dropna`` is set to True, which will exclude NaNs in keys\ndf_dropna.groupby(by=[\"b\"], dropna=True).sum()\nOut[31]: \n     a  c\nb        \n1.0  2  3\n2.0  2  5\n\n# In order to allow NaN in keys, set ``dropna`` to False\ndf_dropna.groupby(by=[\"b\"], dropna=False).sum()\nOut[32]: \n     a  c\nb        \n1.0  2  3\n2.0  2  5\nNaN  1  4\n\nThe default setting of dropna argument is True which means NA are not included in group keys.\n\nGroupBy object attributes\n\nThe groups attribute is a dictionary whose keys are the computed unique groups and corresponding values are the axis labels belonging to each group. In the above example we have:\n\ndf.groupby(\"A\").groups\nOut[33]: {'bar': [1, 3, 5], 'foo': [0, 2, 4, 6, 7]}"]}
{"query": "What is the difference between a list and a tuple in Python?", "relevant_passages": ["Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain a heterogeneous sequence of elements that are accessed via unpacking (see later in this section) or indexing (or even by attribute in the case of namedtuples). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list.\n\nA special problem is the construction of tuples containing 0 or 1 items: the syntax has some extra quirks to accommodate these. Empty tuples are constructed by an empty pair of parentheses; a tuple with one item is constructed by following a value with a comma (it is not sufficient to enclose a single value in parentheses). Ugly, but effective. For example:", "A tuple consists of a number of values separated by commas, for instance:\n\nt = 12345, 54321, 'hello!'\nt[0]\n12345\nt\n(12345, 54321, 'hello!')\n# Tuples may be nested:\nu = t, (1, 2, 3, 4, 5)\nu\n((12345, 54321, 'hello!'), (1, 2, 3, 4, 5))\n# Tuples are immutable:\nt[0] = 88888\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'tuple' object does not support item assignment\n# but they can contain mutable objects:\nv = ([1, 2, 3], [3, 2, 1])\nv\n([1, 2, 3], [3, 2, 1])\nAs you see, on output tuples are always enclosed in parentheses, so that nested tuples are interpreted correctly; they may be input with or without surrounding parentheses, although often parentheses are necessary anyway (if the tuple is part of a larger expression). It is not possible to assign to the individual items of a tuple, however it is possible to create tuples which contain mutable objects, such as lists.", "a = {x for x in 'abracadabra' if x not in 'abc'}\na\n{'r', 'd'}\n5.5. Dictionaries\nAnother useful data type built into Python is the dictionary (see Mapping Types — dict). Dictionaries are sometimes found in other languages as “associative memories” or “associative arrays”. Unlike sequences, which are indexed by a range of numbers, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys. Tuples can be used as keys if they contain only strings, numbers, or tuples; if a tuple contains any mutable object either directly or indirectly, it cannot be used as a key. You can’t use lists as keys, since lists can be modified in place using index assignments, slice assignments, or methods like append() and extend().", "del a\nReferencing the name a hereafter is an error (at least until another value is assigned to it). We’ll find other uses for del later.\n\n5.3. Tuples and Sequences\nWe saw that lists and strings have many common properties, such as indexing and slicing operations. They are two examples of sequence data types (see Sequence Types — list, tuple, range). Since Python is an evolving language, other sequence data types may be added. There is also another standard sequence data type: the tuple.\n\nA tuple consists of a number of values separated by commas, for instance:"]}
{"query": "How does the GIL (Global Interpreter Lock) affect multithreading?", "relevant_passages": ["type), and a buffered loop (for misaligned or incorrect data type situations). Depending on which execution method is\ncalledfor,theloopisthensetupandcomputed.\nFunctioncall\nThis section describes how the basic universal function computation loop is set up and executed for each of the three\ndifferentkindsofexecution. If NPY_ALLOW_THREADS isdefinedduringcompilation,thenaslongasnoobjectarrays\nareinvolved,thePythonGlobalInterpreterLock(GIL)isreleasedpriortocallingtheloops. Itisre-acquiredifnecessary\ntohandleerrorconditions. Thehardwareerrorflagsarecheckedonlyafterthe1-Dloopiscompleted.\nOneloop\nThis is the simplest case of all. The ufunc is executed by calling the underlying 1-D loop exactly once. This is possible\nonly when we have aligned data of the correct type (including byteorder) for both input and output and all arrays have\nuniform strides (eithercontiguous, 0-D, or 1-D). In this case, the 1-D computational loop is called once to compute the", "NumPyUserGuide,Release2.3.0\nCovariancecheckin np.random.multivariate_normal\nARuntimeWarning warningisraisedwhenthecovariancematrixisnotpositive-semidefinite.\nPolynomialClassesnolongertemplatebased\nThe polynomial classes have been refactored to use an abstract base class rather than a template in order to implement\na common interface. This makes importing the polynomial package faster as the classes do not need to be compiled on\nimport.\nMoreGILreleases\nSeveral more functions now release the Global Interpreter Lock allowing more efficient parallelization using the\nthreading module. MostnotablytheGIL isnowreleasedforfancyindexing, np.where andthe random module\nnowusesaper-statelockinsteadoftheGIL.\nMaskedArraysupportformorecomplicatedbaseclasses\nBuilt-inassumptionsthatthebaseclassbehavedlikeaplainarrayarebeingremoved. Inparticular, repr andstr should\nnowworkmorereliably.\nC-API\n12.105.7 Deprecations\nNon-integerscalarsforsequencerepetition"]}
{"query": "Explain the difference between inner joins and outer joins.", "relevant_passages": ["With  data.table : With  dplyr :\n5.12.3  Inner and outer joins\nAn operation that combines columns from two tables is called a join. There are two main types\nof joins: inner joins and outer joins.\nInner joins: create a table containing all observations for which the key appeared in both\ntables. So if we perform an inner join on the  rev_data  and  weather_data  tables using\n DATE  as the key, it won’t contain data for the days that are missing from either the\nrevenue table or the weather table.\nIn contrast, outer joins create a table retaining rows, even if there is no match in the other\ntable. There are three types of outer joins:\nLeft join: retains all rows from the first table. In the revenue example, this means all dates\npresent in  rev_data .\nRight join: retains all rows from the second table. In the revenue example, this means all\ndates present in  weather_data .\nFull join: retains all rows present in at least one of the tables. In the revenue example, this", "dates present in  weather_data .\nFull join: retains all rows present in at least one of the tables. In the revenue example, this\nmeans all dates present in at least one of  rev_data  and  weather_data .\nWe will use the  rev_data  and  weather_data  datasets to exemplify the different types of\njoins. To begin with, we convert them to  data.table  objects (which is optional if you wish to\nuse  dplyr ):\nRemember that revenue data for 2020-03-01 is missing, and weather data for 2020-02-05,\n2020-02-06, 2020-03-10, and 2020-03-29 are missing. This means that out of the 91 days in\nthe period, only 86 have complete data. If we perform an inner join, the resulting table should\ntherefore have 86 rows.\nTo perform an inner join of  rev_data  and  weather_data  using  DATE  as key:\nlibrary(data.table)\nlibrary(dplyr)\nrev_data <- as.data.table(rev_data)\nweather_data <- as.data.table(weather_data)\n05/11/25, 11:07 5 Dealing with messy data | Modern Statistics with R"]}
{"query": "What are Python decorators and when would you use them?", "relevant_passages": ["class C:\n    @staticmethod\n    def f(arg1, arg2, argN): ...\nThe @staticmethod form is a function decorator – see Function definitions for details.\n\nA static method can be called either on the class (such as C.f()) or on an instance (such as C().f()). Moreover, the static method descriptor is also callable, so it can be used in the class definition (such as f()).\n\nStatic methods in Python are similar to those found in Java or C++. Also, see classmethod() for a variant that is useful for creating alternate class constructors.\n\nLike all decorators, it is also possible to call staticmethod as a regular function and do something with its result. This is needed in some cases where you need a reference to a function from a class body and you want to avoid the automatic transformation to instance method. For these cases, use this idiom:\n\ndef regular_function():\n    ...", "@getter\n@setter\n@deleter\nA property object has getter, setter, and deleter methods usable as decorators that create a copy of the property with the corresponding accessor function set to the decorated function. This is best explained with an example:\n\nclass C:\n    def __init__(self):\n        self._x = None\n\n    @property\n    def x(self):\n        \"\"\"I'm the 'x' property.\"\"\"\n        return self._x\n\n    @x.setter\n    def x(self, value):\n        self._x = value\n\n    @x.deleter\n    def x(self):\n        del self._x\nThis code is exactly equivalent to the first example. Be sure to give the additional functions the same name as the original property (x in this case.)\n\nThe returned property object also has the attributes fget, fset, and fdel corresponding to the constructor arguments.\nChanged in version 3.5: The docstrings of property objects are now writeable.\n\n__name__\nAttribute holding the name of the property. The name of the property can be changed at runtime.\n\nAdded in version 3.13.", "class C:\n    @classmethod\n    def f(cls, arg1, arg2): ...\nThe @classmethod form is a function decorator – see Function definitions for details.\n\nA class method can be called either on the class (such as C.f()) or on an instance (such as C().f()). The instance is ignored except for its class. If a class method is called for a derived class, the derived class object is passed as the implied first argument.\n\nClass methods are different than C++ or Java static methods. If you want those, see staticmethod() in this section. For more information on class methods, see The standard type hierarchy.\n\nChanged in version 3.9: Class methods can now wrap other descriptors such as property().\n\nChanged in version 3.10: Class methods now inherit the method attributes (__module__, __name__, __qualname__, __doc__ and __annotations__) and have a new __wrapped__ attribute.\n\nDeprecated since version 3.11, removed in version 3.13: Class methods can no longer wrap other descriptors such as property().", ">>> def implements(np_function):\n... \"Register an __array_function__ implementation for DiagonalArray objects.\"\n... def decorator(func):\n... HANDLED_FUNCTIONS[np_function] = func\n... return func\n... return decorator\n...\nNow we write implementations of numpy functions forDiagonalArray. For completeness, to support the usage\narr.sum() addamethod sum thatcalls numpy.sum(self),andthesamefor mean.\n>>> @implements(np.sum)\n... def sum(arr):\n... \"Implementation of np.sum for DiagonalArray objects\"\n... return arr._i * arr._N\n...\n>>> @implements(np.mean)\n... def mean(arr):\n... \"Implementation of np.mean for DiagonalArray objects\"\n... return arr._i / arr._N\n...\n>>> arr = DiagonalArray(5, 1)\n>>> np.sum(arr)\n5\n>>> np.mean(arr)\n0.2\nIftheusertriestouseanynumpyfunctionsnotincludedin HANDLED_FUNCTIONS,a TypeError willberaisedby\nnumpy, indicating that this operation is not supported. For example, concatenating twoDiagonalArrays does not\nproduceanotherdiagonalarray,soitisnotsupported."]}
{"query": "Write a SQL query to compute a running cumulative sum.", "relevant_passages": ["5.3.3  Summaries of series of numbers\nWhen a  numeric  vector contains a series of consecutive measurements, as is the case, e.g.,\nin a time series, it is often of interest to compute various cumulative summaries. For instance,\nif the vector contains the daily revenue of a business during a month, it may be of value to\nknow the total revenue up to each day, that is, the cumulative sum for each day.\nLet’s return to the  a10  data from Section 4.7, which described the monthly anti-diabetic drug\nsales in Australia during 1991-2008.\nElements 7 to 18 contain the sales for 1992. We can compute the total, highest, and smallest\nmonthly sales up to and including each month using  cumsum ,  cummax , and  cummin :\nIn addition, the  cumprod  function can be used to compute cumulative products.\nAt other times, we are interested in studying run lengths in series, that is, the lengths of runs of\nequal values in a vector. Consider the  upp_temp  vector defined in the code chunk below,", "The following methods on GroupBy act as transformations.\n\nMethod\nDescription\nbfill()\nBack fill NA values within each group\ncumcount()\nCompute the cumulative count within each group\ncummax()\nCompute the cumulative max within each group\ncummin()\nCompute the cumulative min within each group\ncumprod()\nCompute the cumulative product within each group\ncumsum()\nCompute the cumulative sum within each group\ndiff()\nCompute the difference between adjacent values within each group\nffill()\nForward fill NA values within each group\npct_change()\nCompute the percent change between adjacent values within each group\nrank()\nCompute the rank of each value within each group\nshift()\nShift values up or down within each group\nIn addition, passing any built-in aggregation method as a string to transform() (see the next section) will broadcast the result across the group, producing a transformed result. If the aggregation method has an efficient implementation, this will be performant as well."]}
{"query": "Explain lazy evaluation and give an example in Python.", "relevant_passages": ["Another key feature is that the local variables and execution state are automatically saved between calls. This made the function easier to write and much more clear than an approach using instance variables like self.index and self.data.\n\nIn addition to automatic method creation and saving program state, when generators terminate, they automatically raise StopIteration. In combination, these features make it easy to create iterators with no more effort than writing a regular function.\n\n9.10. Generator Expressions\nSome simple generators can be coded succinctly as expressions using a syntax similar to list comprehensions but with parentheses instead of square brackets. These expressions are designed for situations where the generator is used right away by an enclosing function. Generator expressions are more compact but less versatile than full generator definitions and tend to be more memory friendly than equivalent list comprehensions.\n\nExamples:", "The Boolean operators and and or are so-called short-circuit operators: their arguments are evaluated from left to right, and evaluation stops as soon as the outcome is determined. For example, if A and C are true but B is false, A and B and C does not evaluate the expression C. When used as a general value and not as a Boolean, the return value of a short-circuit operator is the last evaluated argument.\n\nIt is possible to assign the result of a comparison or other Boolean expression to a variable. For example,\n\nstring1, string2, string3 = '', 'Trondheim', 'Hammer Dance'\nnon_null = string1 or string2 or string3\nnon_null\n'Trondheim'\nNote that in Python, unlike C, assignment inside expressions must be done explicitly with the walrus operator :=. This avoids a common class of problems encountered in C programs: typing = in an expression when == was intended."]}
{"query": "What is the difference between batch processing and streaming?", "relevant_passages": ["Optimize parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\nOther benefits include:", "### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python  theme={null}\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n}, stream_mode=\"values\"):\n    # Each chunk contains the full state at that point\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"Agent: {latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n```\n\n<Tip>\n  For more details on streaming, see [Streaming](/oss/python/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/python/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:"]}
{"query": "Write Python code to implement a custom exception class.", "relevant_passages": ["Exception classes can be defined which do anything any other class can do, but are usually kept simple, often only offering a number of attributes that allow information about the error to be extracted by handlers for the exception.\n\nMost exceptions are defined with names that end in “Error”, similar to the naming of the standard exceptions.\n\nMany standard modules define their own exceptions to report errors that may occur in functions they define.\n\n8.7. Defining Clean-up Actions\nThe try statement has another optional clause which is intended to define clean-up actions that must be executed under all circumstances. For example:\n\ntry:\n    raise KeyboardInterrupt\nfinally:\n    print('Goodbye, world!')"]}
{"query": "How do you profile Python code to find bottlenecks?", "relevant_passages": ["[package.dependencies]\nmdurl = \">=0.1,<1.0\"\n\n[package.extras]\nbenchmarking = [\"psutil\", \"pytest\", \"pytest-benchmark\"]\ncode-style = [\"pre-commit (>=3.0,<4.0)\"]\ncompare = [\"commonmark (>=0.9,<1.0)\", \"markdown (>=3.4,<4.0)\", \"mistletoe (>=1.0,<2.0)\", \"mistune (>=2.0,<3.0)\", \"panflute (>=2.3,<3.0)\"]\nlinkify = [\"linkify-it-py (>=1,<3)\"]\nplugins = [\"mdit-py-plugins\"]\nprofiling = [\"gprof2dot\"]\nrtd = [\"jupyter_sphinx\", \"mdit-py-plugins\", \"myst-parser\", \"pyyaml\", \"sphinx\", \"sphinx-copybutton\", \"sphinx-design\", \"sphinx_book_theme\"]\ntesting = [\"coverage\", \"pytest\", \"pytest-cov\", \"pytest-regressions\"]"]}
{"query": "Explain the concept of immutability in Python and where it matters.", "relevant_passages": ["9.1. A Word About Names and Objects\nObjects have individuality, and multiple names (in multiple scopes) can be bound to the same object. This is known as aliasing in other languages. This is usually not appreciated on a first glance at Python, and can be safely ignored when dealing with immutable basic types (numbers, strings, tuples). However, aliasing has a possibly surprising effect on the semantics of Python code involving mutable objects such as lists, dictionaries, and most other types. This is usually used to the benefit of the program, since aliases behave like pointers in some respects. For example, passing an object is cheap since only a pointer is passed by the implementation; and if a function modifies an object passed as an argument, the caller will see the change — this eliminates the need for two different argument passing mechanisms as in Pascal.", "a = {x for x in 'abracadabra' if x not in 'abc'}\na\n{'r', 'd'}\n5.5. Dictionaries\nAnother useful data type built into Python is the dictionary (see Mapping Types — dict). Dictionaries are sometimes found in other languages as “associative memories” or “associative arrays”. Unlike sequences, which are indexed by a range of numbers, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys. Tuples can be used as keys if they contain only strings, numbers, or tuples; if a tuple contains any mutable object either directly or indirectly, it cannot be used as a key. You can’t use lists as keys, since lists can be modified in place using index assignments, slice assignments, or methods like append() and extend().", "A tuple consists of a number of values separated by commas, for instance:\n\nt = 12345, 54321, 'hello!'\nt[0]\n12345\nt\n(12345, 54321, 'hello!')\n# Tuples may be nested:\nu = t, (1, 2, 3, 4, 5)\nu\n((12345, 54321, 'hello!'), (1, 2, 3, 4, 5))\n# Tuples are immutable:\nt[0] = 88888\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: 'tuple' object does not support item assignment\n# but they can contain mutable objects:\nv = ([1, 2, 3], [3, 2, 1])\nv\n([1, 2, 3], [3, 2, 1])\nAs you see, on output tuples are always enclosed in parentheses, so that nested tuples are interpreted correctly; they may be input with or without surrounding parentheses, although often parentheses are necessary anyway (if the tuple is part of a larger expression). It is not possible to assign to the individual items of a tuple, however it is possible to create tuples which contain mutable objects, such as lists.", "Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain a heterogeneous sequence of elements that are accessed via unpacking (see later in this section) or indexing (or even by attribute in the case of namedtuples). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list.\n\nA special problem is the construction of tuples containing 0 or 1 items: the syntax has some extra quirks to accommodate these. Empty tuples are constructed by an empty pair of parentheses; a tuple with one item is constructed by following a value with a comma (it is not sufficient to enclose a single value in parentheses). Ugly, but effective. For example:"]}
{"query": "What is informed consent in the context of data collection?", "relevant_passages": ["of persons before their data is gathered. There is further discussion in \nthe section below, but many data companies also release details about \nexactly how personal data will be used and for what purposes. Consent \nand transparency are usually understood to be important barriers for data \ncollection, and proper data governance would, where other business and \nsocietal needs do not clash, look to obtain that consent and be transparent \nto data subjects about what happens with their information. \nMore obviously, the storage of data is an important feature of data \ngovernance. Exacting security measures must be in place, and be routinely \ntested, to ensure a much greater range of harms do not occur. There are \nthe same informational considerations as with collection; simply knowing \nthat one’s information is stored in a location one cannot control is \nworrying. But the disclosure of that information to the wrong parties,", "Data Privacy and Ethical Consideration in Data Science \n  Published By: Fifth Dimension Research Publication                   https://fdrpjournals.org/                                         144 | P a g e  \n \npurpose. Collecting excessive or irrelevant data increases the risk of privacy violations and data breaches. Purpose limitation goes \nhand-in-hand with this, ensuring that data is used only for the purpose it was originally collected for, and not for unrelated or \nunauthorized purposes. Both principles are core components of privacy regulations such as GDPR.  \n \n6.4 Informed Consent and User Control \n  Obtaining informed consent is a cornerstone of data privacy. It requires organizations to be transparent  about how they \ncollect, use, and share personal data. Individuals must be given clear, concise information about how their data will be used and \nshould be able to opt-in or opt-out of data collection processes.", "12\nThe consent process upholds ethical principles \nof autonomy and freedom of choice by allowing \npatients to make a well-informed decision. \nWhile the consent process is arguably imperfect \nand limited, it has been the best available tool \nfor maintaining the aforementioned ethical \nprinciples.\n13 However, given that Big Data \nresearch diﬀ  ers drastically from traditional \nresearch in terms of subject participation, one \nof the biggest concerns is whether current \nresearch requirements regarding consent still \nadequately protect patients’ autonomy. \nThe Revised Common Rule emphasizes \nthe importance of proper consent but does \nnot adequately extend its reach to Big Data \nresearch. The new standards emphasize the \nimportance of proper consent by requiring \nresearchers to succinctly state to patients what \nthey need to know in regard to why they would \nor would not want to participate in a protocol \nare the very beginning of every informed \nconsent document.\n7 Before the enactment of"]}
{"query": "How do you anonymize personal data effectively?", "relevant_passages": ["VI.DATA PRIVACY \nData Privacy in Data Science \nData privacy is one of the most significant ethical considerations in data science. As data science relies on vast amounts of \ndata, much of it personal or sensitive, ensuring that privacy is preserved is essential. In this context, privacy refers to the protection \nof individuals' data from unauthorized access, misuse, or exploitation.  \n  \n6.1Personal Data and Data Anonymization \nPersonal data includes any information that can identify an individual, such as names, email addresses, social security \nnumbers, and even IP addresses. To protect individual privacy, data scientists often rely on anonymization and pseudonymization \ntechniques. \n Anonymization:  \n  This process removes personally identifiable information (PII) from a dataset, making it impossible to trace data back to \nan individual. However, true anonymization  can be challenging to achieve, as sophisticated techniques may still allow for re-", "an individual. However, true anonymization  can be challenging to achieve, as sophisticated techniques may still allow for re-\nidentification of individuals by correlating data points with other datasets.  \n \n Pseudonymization:  \n  In this method, identifiable information is replaced with pseudonyms, allowing for data to be linked to individuals without \ndirectly revealing their identities. While pseudonymization offers privacy protection, it is not as strong as anonymization, as it \nallows for the possibility of re-linking the data to individuals under certain conditions. \n  \n  6.2 Differential Privacy \n  Differential privacy is an advanced privacy-preserving technique that allows data scientists to analyze datasets and derive \ninsights without revealing information about individual data points. This technique ensures that the inclusion or exclusion of any", "Privacy is a crucial aspect of trying to achieve the conﬁdentiality of data. \nMaintaining conﬁdentiality can be a daunting task that requires many \ntechniques to ensure the privacy of data. One area of interest, anonymity, \nmanipulates data in a way that obfuscates the identity of individuals but \nstill allows others to view the applicable data. Keeping data private can be \naccomplished through a procedure known as k-anonymity. \nK-anonymity: The idea behind K-anonymity has been around since \n1986 when Tore Dalenius wrote a paper describing how to identify \npersonal information from census data. The theory behind k-anonymity \nis that people’s personal information can be anonymized, which preserves \nprivacy but still allows pertinent data to be shared for scientiﬁc studies. \nFor example, if a survey for the cure of cancer was conducted by a", "individual. Once the data is categorized, one can apply one of two \nmethods to the data to anonymize the dataset. \nThe ﬁrst method is to generalize the data. This strategy can be done by \nlooking through all the data in a particular column and ﬁnding common-\nalities. If one category is age, the data can be grouped into ﬁve- or \nten-year increments. The objective here would allow only pertinent infor-\nmation to be shown while preserving key characteristics of the subject. \nThe other method is to sanitize the information from the released data. \nThere will be times when the data cannot be generalized or is not \nneeded for a different study or research. In these situations, the data \nshould not be included in the released dataset. Examples of this are the \nperson’s name, street address, and even the person’s religion. This type \nof information can be used to identify a person relatively easily. \nThe key feature behind k-anonymity is to have anonymity set to at least", "nized best practices, such as those outlined in NIST SP 800-53r5, in order \nto protect their customers’ information. Individuals should also take an \nactive role in managing the way companies and organizations collect, use, \nshare, and sell their data by reviewing and understanding various terms \nand conditions presented to them. Additionally, users should adopt best \npractices to limit their exposure while navigating online, using IoT devices \nin their homes and ofﬁces, and sharing information with organizations. \nResearchers and organizations that have a need to share user data with \nafﬁliates and partners should investigate methods such as K-Anonymity \nand other methods to obfuscate personal information. Connected devices \nare becoming more proliﬁc and companies will continue to collect, \nprocess, and utilize personal information for activities such as targeted \nadvertisements and tailored user experiences. While these activities can"]}
{"query": "What is differential privacy and why is it important?", "relevant_passages": ["Differential privacy \nstarts with a mathemat-\nical definition of privacy \nloss that is based on an \nintuitive understanding \nof privacy: your privacy \nisn’t violated if your \nconfidential data are  \nnot used to produce a \nstatistic.\nDownloaded from http://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf by guest on 04 November 2025", "Another aspect of differential privacy is its transparency, \na notion that comes from DP’s roots in computer security \nand cryptography. The deep idea is that the best way to \ncreate a secure system is to use open design principles; se-\ncurity should come from the strength of the design rather \nthan from its secrecy.\nThis principle was first formulated by Dutch cryp-\ntographer Auguste Kerckhoffs in 1883. One of the best \nexamples of its use today is the development and imple-\nmentation of the Advanced Encryption Standard by the \nDownloaded from http://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf by guest on 04 November 2025", "Consumer Privacy Act, respectively, and US regulators at \nagencies such as the Federal Trade Commission, Federal \nCommunications Commission, and Consumer Financial \nProtection Bureau have begun insisting that companies \nrespect the privacy concerns of their users.\nOfficial Differential Privacy\nThe US Census Bureau was the first official statistics agency  \nto adopt differential privacy; it will continue to do so, and it \nwon’t be the last. A growing number of government agen -\ncies are experimenting with differential privacy to protect \nDownloaded from http://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf by guest on 04 November 2025", "1\nCONCEPTS AND THEORIES\nThe last chapter provided a broad overview of differential \nprivacy with a simple example of database reconstruction \nand the historical context of the US Census. This chapter \nwill drill down on those concepts. It will involve a little \nmath, but not much.\nDP in a Nutshell\nTo start with, here is my take on the core concept of dif -\nferential privacy:\nAdding noise to the result of every database query is \nthe only way to provide for composable confidentiality \nprotection in a statistical database. Differential privacy \nis a mathematical framework based on a definition \nDownloaded from http://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf by guest on 04 November 2025"]}
{"query": "Describe common sources of bias in machine learning datasets.", "relevant_passages": ["labels in addition to the possible inaccurate classi ﬁcation. In this way, various\nchoices of de ﬁned class labels result in different consequences and impact certain\nclasses (Barocas & Selbst, 2014; Gandy, 2010; Hildebrandt & Koops, 2010).\nAnother potential area of algorithm bias results from the actual training of the data\nthat machine learning works with. Biased training data can lead to discriminatory\nmodels. Hence, in situations where biased cases are used as training data, the\nproblem ends up repeating and reproducing itself. On the other hand, if the bias\nlies in the sample, the algorithm will repeat, eventually putting the underrepresented\npopulation at a disadvantage. Both cases can affect the training data, the algorithm,\nthe results, and, in turn, its derived implications (Custers, 2013).\nIn addition, the training data and algorithm can adopt biases based on user or\nconsumer behavior. Results from a study by Sweeney ( 2013) show that Google is", "communities differs. \nBias and discrimination as studied by the AI community \nBias and discrimination are heavily studied in AI policy  circles, and are cited as major concerns for \ngenerative AI, due to the increase in scale and scope of potential algorithmic bias resulting from foundation \nmodels and the massive training data they use (OECD, 2023[13]; Lorenz, Perset and Berryhill, 2023 [12]). \nThe work of the AI community has brought to light the many sources of algorithmic bias (OECD, 2023[14]), \nincluding: historical bias, representations bias , measurement bias , methodological and evaluation bias , \nmonitoring bias and skewed samples , feedback loops and popularity bias (OECD, 2023[14]). AI scholars \nhave also identified the characteristics and incompatibilities between different forms of fairness or non -\ndiscrimination, including equality of opportunity, equality of outcome or statistical parity, and counterfactual", "Optimal coordination of privacy and AI approaches to non -discrimination can be hampered by definitional \nand terminological issues.  \nFairness in AI discourse \nFor AI communities, fairness is often understood to refer to outcomes from the application of AI (such as \npredictions, recommendations, or decisions) that are based on algorithms and datasets with consideration \nfor bias, for example, through eliminating algorithm ic or dataset bias for specific groups (e.g. those \ncategorised by class, gender, race, or sexual orientation). Bias is a systematic (as opposed to a random) \nerror, associated with certain categories of data inputs. For example, a facial recognition algorit hm may \nmake more errors for people wearing glasses than for people without glasses. A difference in error rates \nbetween people who wear glasses versus people without glasses can be problematic from a technical and", "The ethical challenges associated with data science go beyond privacy to encompass a broader set of concerns. These \ninclude issues of fairness, accountability, and transparency (FAT), which are frequently addressed in recent studies.  \n \n  \n3.1 Fairness:  \n  One of the primary ethical concerns is the risk of bias in data and algorithms. Machine learning algorithms can \nunintentionally perpetuate existing societal biases if they are trained on biased data. Studies, such as those by Barocas and Selbst \n(2016), discuss how algorithmic decision-making can lead to unfair treatment of certain groups, particularly in domains like hiring, \ncriminal justice, and lending. Researchers have proposed various fairness metrics and interventions, such as fairness constraints \nand adversarial debiasing, to mitigate these effects (Zemel et al., 2013). \n \n3.2 Accountability:  \n  The issue of accountability is also central to the ethics of data science. Pasquale (2015) has raised concerns about the"]}
{"query": "What are the risks of using AI for automated decision-making?", "relevant_passages": ["absence of biases or discriminatory practices related to race, gender, or ethnicity. \nResponsibility. When AI systems make decisions that have terrible consequences, such as the loss of money, well-being, \nor even human lives, the division of blame within society is still being worked out. A multidisciplinary approach involving \nlawyers, regulators, AI technologists, ethics committees,  and   the   general   public  should   ensure  accountability for the \nresults of AI-powered decisions. Finding the best balance in circumstances where an AI system is potentially safer than the \nhuman  action  it  is  duplicating  but  nonetheless  poses  problems  is  one  challenge.  This  involves  assessing  the  benefits  of \nautonomous driving systems that do cause fatalities, albeit far fewer than those brought on by human intervention.", "22  AI, DATA GOVERNANCE & PRIVACY: SYNERGIES AND AREAS OF INTERNATIONAL CO-OPERATION  \n \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n      \nBox 1.1. Real and potential risks associated with AI systems \nThe OECD has worked to identify real and potential risks associated with AI systems, including \ngenerative AI, across its workstreams. Some risks are listed below:  \n• The amplification of mis- and dis-information at a large scale and scope, particularly through \ncreation of artificial content that humans mistake for real content;  \n• AI model “hallucinations” that give incorrect or non-factual responses in a credible way, or the \ngeneration of illicit images such as fake child sexual exploitation material (e.g. “fake nudes”)  ; \n• Harmful bias and discrimination at an increased scale;  \n• Risks to privacy and data governance, at the level of training data, at the model  level, at the \nintersection of data and model levels, or at the human-AI interaction level;", "time discussing Big Data research risks would \nbe shared.\n44 \nAt this time, other risks from Big Data \nresearch and its applications might still be \nlargely unknown. AI and machine learning \ntechniques are beginning to outstrip our \ncapacity to understand how conclusions \nare reached.\n45 Digital computer programs \ncan arrive at conclusions using incredible \nspeed and complexity, such that even their \ndevelopers cannot understand how the \ninputs were used by the program to arrive at \na given conclusion.\n46 These algorithms have", "The above case studies underscore the real-world ethical risks associated with AI in hiring, finance, law enforcement, and privacy. \nAI biases can exacerbate economic inequality, racial discrimination, and privacy violations, making it critical to adopt a multifaceted \napproach to AI ethics. Addressing these dilemmas requires: \n• Algorithmic Fairness Techniques: Developing bias-aware machine learning models. \n• Regulatory Enforcement: Strengthening compliance with AI fairness and transparency laws. \n• Explainability and Audits: Implementing XAI techniques to improve AI accountability.  \nBy integrating responsible AI design principles, policymakers and engineers can mitigate algorithmic harms and e nsure AI-driven \ndecision-making prioritizes fairness and societal well-being.", "about their use of advanced analytics focused on the valuable insights that it produces. \nThe survey and interviews suggested that this rosy view was only part of the picture \nand that some of the more sophisticated companies, at least, also recognized the very \nreal threats that these activities create.\n1 Others have published lists of risks from AI (Future of Privacy Forum 2017). For the most part, \nthey align with those that our respondents identiﬁed."]}
{"query": "What is the “right to be forgotten”?", "relevant_passages": ["104 K. D. SCHUBERT AND D. BARRETT\n“right to be forgotten.” That is, they maintain that an important part of \nprivacy is the ability to have one’s past (decisions, actions, events) remain \noverlooked. The GDPR speciﬁcally includes Article 17 (Art. 17 GDPR, \nn.d.) that states the right of data subjects to demand that controllers \nerase their personal data under certain conditions. The GDPR requires \na high level of openness about the data being collected and how it is \nprocessed. Article 15 (Art. 15 GDPR, n.d.) states that data subjects have \na right of access, to know the content of the data, the purposes of its \nprocessing, who it has been or will be shared with, how long it will be \nshared, whether any automated processing of that data will occur, and \neven access to meaningful information about how the automation works \nand what conclusions are hoped to be drawn from it. As discussed, many \nsee a dimension of privacy as covering our own personal autonomy—", "focus on ensuring that the best possible data governance is put into place, \nenforced, audited, managed, and evolved to ensure the data privacy and \nethical use of data we should all expect. \nReferences \nArt. 15 GDPR. (n.d.). Right to Erasure (‘Right to be Forgotten’). General Data \nProtection Regulation (GDPR). Retrieved August 22, 2023. \nArt. 17 GDPR. (n.d.). Right to Erasure (‘Right to be Forgotten’). General Data \nProtection Regulation (GDPR). Retrieved August 22, 2023."]}
{"query": "How can transparency be ensured in algorithmic decision-making?", "relevant_passages": ["38  AI, DATA GOVERNANCE & PRIVACY: SYNERGIES AND AREAS OF INTERNATIONAL CO-OPERATION  \n \nOECD ARTIFICIAL INTELLIGENCE PAPERS \n      \nThe p rivacy community is increasingly focused on AI explainability to ensure accuracy, fairness, and \naccountability in data processing . Privacy and data protection laws and PEAs use the broad term \n“transparency”, a term that for them subsumes explainability and interpretability  of data processing \npractices. Courts and  PEAs may impose explainability for algorithmic decisions that have imp ortant \nimpacts on individuals, since without explainability, human oversight cannot be assured, and individuals \ncannot have an effective right of redress (CJEU, 2022 [69]). Modern privacy laws have recognised \ntransparency as the most effective technique for oversight and accountability. Arguably, some say that the \nreal paradox of privacy could be that “privacy requires transparency” (Rotenberg, 2021, p. 497[70]), i.e., the", "However, when it comes to helping persons affected by AI systems  understand and contest their \nprocesses or outputs, or to help users detect algorithmic discrimination, data protection law and AI policy \nalign. \nThe same alignment is evident in transparency and traceability. Transparency rais es questions on how \ninformation on complex systems can be made easily accessible, understandable, and still complete, a \nsubject long studied by privacy authorities. Traceability, is a common concern for AI and privacy policy \ncommunities where coordination should be encouraged. \nPrinciple 1.4: Robustness, security and safety \nGenerative AI’s complexity and opacity may complicate efforts to constrain model behavio ur (Lorenz, \nPerset and Berryhill, 2023 [12]), thereby hurting reliability. Homogenisation (Bommasani, 2021 [73]) and \n“algorithmic monoculture” risk, which refers to the risk of running similar algorithms with similar", "Ensuring fairness in data science involves mitigating biases that can be introduced by algorithms or by the data itself. \nWhen data used to train models is biased, the resulting algorithms may perpetuate or even amplify existing inequalities. For \nexample, in hiring  algorithms, biased data may lead to discrimination  against certain demographic groups. Fairness-aware \nalgorithms are designed to ensure that model outcomes are equitable across different population segments, addressing issues like \nrace, gender, and socioeconomic status. \n  \n4.1 Transparency and Interpretability \n  Transparency refers to the openness with  which data science systems, particularly  algorithms, operate. In complex \nmachine learning models, such as deep learning, the decision -making process can be opaque, making it difficult to understand \nhow certain outcomes are derived. This \"black box\" issue undermines trust in these systems. To counter this, responsible data", "individuals concerned.  Discrimination is one form of unfairness, but it is not the only one. \nTransparency \nand \nexplainability \nFor AI policy communities, transparency, explainability and interpretability have different meanings but overall refer to the good \npractice of AI actors providing accessible information to users to foster a general understanding of AI systems, making \nstakeholders aware of their interactions with AI systems, enabling those affected by an AI system to understand the outcome, \nand to enable those affected by an AI system to challenge its outcome based on plain and easy-to-understand information that \nserved as the basis for the prediction, recommendation or decision.  \n \nFor privacy policy communities, transparency is a positive legal obligation to inform individuals, from whom personal data are \ncollected, no later than at the time of data collection, on the use purposes for which consent is requested and the subsequent", "features of personal data that are more likely to influence the product feature (e.g. whether movie was \nviewed completely, viewed multiple times) (Singapore Data Protection Commission, 2024[65]). \nIt can be a challenge to make highly complex data processing information understandable to individuals, \nparticularly since many simply click through the  transparency notification  information as quickly as \npossible. This challenge is further complicated  by the \"black box\" problem – where AI developers are \nunable to fully explain how an AI system came to generate an output – and the trade-off between accuracy \nand interpretability of AI models.  The overarching objective of transparency requirements, however, is to \nguarantee individual participation, and human agency to consent to disclosure or control of facets of their \ndata and related identities (e.g. body, data, reputation). This objective aligns with the objective of \ntransparency for AI (Tabassi, 2023[51])."]}
{"query": "What is model fairness and how do you measure it?", "relevant_passages": ["Ensuring fairness in data science involves mitigating biases that can be introduced by algorithms or by the data itself. \nWhen data used to train models is biased, the resulting algorithms may perpetuate or even amplify existing inequalities. For \nexample, in hiring  algorithms, biased data may lead to discrimination  against certain demographic groups. Fairness-aware \nalgorithms are designed to ensure that model outcomes are equitable across different population segments, addressing issues like \nrace, gender, and socioeconomic status. \n  \n4.1 Transparency and Interpretability \n  Transparency refers to the openness with  which data science systems, particularly  algorithms, operate. In complex \nmachine learning models, such as deep learning, the decision -making process can be opaque, making it difficult to understand \nhow certain outcomes are derived. This \"black box\" issue undermines trust in these systems. To counter this, responsible data", "To systematically identify and quantify bias, we implement the Fairness-Aware Bias Detection Algorithm  (Algorithm 1). This \nalgorithm takes a trained model and evaluates it on test data while considering a sensitive attribute. It computes the rates of positive \noutcomes for different demographic groups and measures discrepancies in prediction fairness. The results guide interventions to \nimprove fairness and minimize discriminatory effects. \n \nAlgorithm 1 Fairness-Aware Bias Detection Algorithm \n \n1: Input: Trained model, test dataset, sensitive attribute \n2: Output: Bias metrics (Disparate Impact, Equalized Odds) \n3: Generate predictions using the trained model \n4: Compute the proportion of positive predictions for different demographic groups  \n5: Calculate the ratio of positive outcomes (Disparate Impact) \n6: Compare true positive and false positive rates across groups (Equalized Odds)  \n7: Return bias metrics for analysis \n \n4.3 Explainable AI (XAI) for Model Transparency", "• Mathematical and Algorithmic Formulation for Fair AI: We introduce formal mathematical formulations for bias \ndetection metrics such as Disparate Impact (DI) and Equalized Odds (EO) and propose algorithmic implementations of \nfairness-aware AI models. This provides a structured approach to integrating fairness constraints into machine learning \npipelines. \n• Recommendations for Ethical AI Governance: We propose a roadmap for ethical AI adoption, including bias-aware model \nevaluation, XAI -driven transparency, and interdisciplinary AI ethics governance. Our study bridges the gap between \ntheoretical AI ethics discussions and practical deployment strategies.", "International Journal of Current Science Research and Review \nISSN: 2581-8341    \nVolume 08 Issue 03 March 2025    \nDOI: 10.47191/ijcsrr/V8-i3-09, Impact Factor: 8.048   \nIJCSRR @ 2025   \n \nwww.ijcsrr.org \n   \n1071   *Corresponding Author: Chiranjeevi Bura                                                      Volume 08 Issue 03 March 2025 \n                   Available at: www.ijcsrr.org \n                                                              Page No. 1067-1078 \n \n• Counterfactual Fairness (CF) : A model is deemed fair if changing a sensitive attribute (such as gender or ethnicity) does \nnot alter its predictions. This concept helps in assessing whether an AI system makes decisions independently of protected \nattributes. \n4.2 Bias Detection Algorithm \nTo systematically identify and quantify bias, we implement the Fairness-Aware Bias Detection Algorithm  (Algorithm 1). This", "Optimal coordination of privacy and AI approaches to non -discrimination can be hampered by definitional \nand terminological issues.  \nFairness in AI discourse \nFor AI communities, fairness is often understood to refer to outcomes from the application of AI (such as \npredictions, recommendations, or decisions) that are based on algorithms and datasets with consideration \nfor bias, for example, through eliminating algorithm ic or dataset bias for specific groups (e.g. those \ncategorised by class, gender, race, or sexual orientation). Bias is a systematic (as opposed to a random) \nerror, associated with certain categories of data inputs. For example, a facial recognition algorit hm may \nmake more errors for people wearing glasses than for people without glasses. A difference in error rates \nbetween people who wear glasses versus people without glasses can be problematic from a technical and"]}
{"query": "Explain the challenges of balancing privacy and utility in data analysis.", "relevant_passages": ["insights without revealing information about individual data points. This technique ensures that the inclusion or exclusion of any \nsingle data point does not significantly affect the outcome of an analysis, making it difficult to infer specific information about \nindividuals. \nFor example, companies like Apple and Google have implemented differential privacy in their systems to analyze user behavior \nwhile safeguarding personal information. Differential privacy is increasingly being adopted by both private organizations and \ngovernmental institutions to balance the need for data analysis with individual privacy. \n \n6.3 Data Minimization and Purpose Limitation \n  Data minimization is a privacy principle that mandates organizations to collect only the data that is necessary for a specific", "Data Privacy and Ethical Consideration in Data Science \n  Published By: Fifth Dimension Research Publication                   https://fdrpjournals.org/                                         140 | P a g e  \n \nelusive (Bamberger & Mulligan, 2015). \n \n2.2 Privacy-Preserving Techniques:  \n  On the technical side, data privacy researchers have explored various methods to protect individual privacy while enabling \ndata analysis. Anonymization, differential privacy, and encryption are key techniques discussed in the literature (Dwork & Roth, \n2014). Differential privacy, in particular, has gained traction as a method that allows organizations to perform data analysis while \nproviding mathematical guarantees that individual data points cannot be reverse-engineered (Abowd, 2018). \n \nIII.ETHICAL CONCERNS IN DATA SCIENCE \nThe ethical challenges associated with data science go beyond privacy to encompass a broader set of concerns. These", "What’s needed is a new \ngeneration of laws that \nconsider our modern \nunderstanding of \nprivacy: that every data \nrelease may potentially \nhave problems, and  \na balance is crucial \nbetween data utility  \nand privacy.\nDownloaded from http://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf by guest on 04 November 2025", "Preface   xix\nfor delivering public services, redistricting, and academic \nresearch— this, despite the fact that the amount of noise \nthat DP adds is tunable. They also maintained that while \nlegacy approaches developed for statistical disclosure limi-\ntation lack DP’s mathiness, those legacy approaches may do \na better job balancing privacy protection and data utility.3\nWriting in the Harvard Data Science Review, the highly \nrespected academics Daniel L. Oberski and Frauke Kreuter \npresent a view somewhere in the middle:\nThe main disadvantage of ensuring differential \nprivacy is that it typically requires more noise \ninfusion than traditional techniques. This is a \nconsequence of the fact that traditional techniques \nonly need to prevent linkage, while differential \nprivacy prevents linkage through reconstruction. \nOne might expect that in the discussion on how and \nwhen differential privacy should be applied, level- \nheaded experts convene to weigh such pros and cons", "Differential privacy (DP) is one promising way to offer privacy\nalong with open access, but further inquiry is needed into the\ntensions between DP and data science. In this study, we conduct\ninterviews with 19 data practitioners who are non-experts in DP as\nthey use a DP data analysis prototype to release privacy-preserving\nstatistics about sensitive data, in order to understand perceptions,\nchallenges, and opportunities around using DP. We find that while\nDP is promising for providing wider access to sensitive datasets,\nit also introduces challenges into every stage of the data science\nworkflow. We identify ethics and governance questions that arise\nwhen socializing data scientists around new privacy constraints\nand offer suggestions to better integrate DP and data science.\nCCS CONCEPTS\n• Security and privacy →Social aspects of security and pri-\nvacy; Usability in security and privacy ; Privacy-preserving pro-\ntocols.\nKEYWORDS"]}
{"query": "What are the ethical concerns around facial recognition technology?", "relevant_passages": ["consent, resulting in ethical and privacy concerns. \nb) Data Security Issues:  Hacking and data breaches have shown that the system isn't completely secure, allowing \npeople to see into people's private areas without their permission.  \nc) Public Backlash : The case led to criticism from the public and demands for more control. It emphasized the \nnecessity of open procedures and user discretion when it comes to sharing surveillance videos.  \nd) Clear view AI and Facial Recognition: \nPrivacy Challenge \nClear view AI's face recognition technology, which extracts publicly accessible photographs to construct an extensive \ndatabase, has prompted apprehensions regarding privacy infringement and the possibility of extensive surveillance.  \n          International Research Journal of Engineering and Technology (IRJET)      e-ISSN: 2395-0056", "International Research Journal of Engineering and Technology (IRJET)     e-ISSN: 2395-0056 \n                Volume: 11 Issue: 05 | May 2024              www.irjet.net                                                                        p-ISSN: 2395-0072 \n  \n© 2024, IRJET       |       Impact Factor value: 8.226       |       ISO 9001:2008 Certified Journal       |     Page 784 \n \nOutcomes and Consequences \na) Unregulated Surveillance : Transparent the technology of AI has facilitated unregulated and widespread \nsurveillance, which has significant concerns for human privacy. Unbeknownst to them, individuals' photos were \nutilized for facial recognition purposes without their knowledge or consent. \nb) Law Enforcement Utilization:  Law enforcement authorities used the technology to identify people, which \nsparked discussions about how to strike a balance between private rights and public safety.", "of inferences to be made should warrant the \nutmost concern. \nAs highlighted in the previous paragraph, \nmany participants might be unaware of the \ncapabilities of analytics. Many people are likely \nunaware of the fact that facial features have \na degree of correlation to sexual orientation, \nand that AI technologies can recognize this. \nAs technological capabilities rapidly advance, \nit becomes more and more likely that people \nwill be unable to foresee the ways in which \ntheir publicly available information could be \nused. In almost all medical research, informing \nparticipants so they can choose whether or not \nto be in a protocol is an absolute requirement, \nsince anything less than this is viewed as an \nunacceptable violation of human dignity.\n7 With \ncurrent regulations allowing for lenient consent \nrequirements for Big Data research, these \nviolations of human dignity seem eminent.\nA balanced remedy that fosters innovative \nBig Data research and maintains the autonomy", "conce Pts  and theor Ies   113\ntasks— and often the data are highly personal. A major \nrisk in machine learning is that an attacker with access to \nthe model can sometimes learn confidential information \nabout the individuals whose data was used to train the \nmodel. Differential privacy can reduce this risk.\nFor example, consider a system that uses machine \nlearning for face recognition. By analyzing the faces of mil-\nlions of people, with several samples of each person, the \nsystem learns the kinds of differences between faces that \nsignify different people—  like e yes that are round versus \noval— and the kinds of differences that are inconsequen -\ntial, such as the direction that a person’s eyes might be \npointing. It’s then possible to use that classifier to deter -\nmine if two photographs came from the same person or \ndifferent people. This is the core idea behind nearly every \nmodern system that identifies people based on their faces.", "While employers note that the use of these technologies can warn \nemployees about, and even preempt, pending hazards and medical \nconditions, unsurprisingly employees across the various facets of order \nfulﬁllment operations have expressed an aversion to the use of some \nof these surveillance and tracking technologies (Wendt, \n2023). These \nemployees argue that these technologies can infringe on their privacy \nrights and often put them at risk of being the recipients of unfair, \nand sometimes unwarranted, punitive actions. Of note, employees and \nother stakeholders are anxious about how employers will collect and use \nbiometric data (Guillot, \n2019; Sheng, 2019) or use data that presents \nemployee performance at a more detailed level. Further, there is substan-\ntial concern over the autonomous decision-making authority that these \ntechnologies will have in order fulﬁllment operations as they become \nmore advanced and pervasively used (Dohrmann et al., \n2022; Iyer, 2023)."]}
{"query": "How can proxy variables inadvertently encode sensitive data?", "relevant_passages": ["ﬁed by linking public domain anonymized data sets. At\nthe very least, this involves invasion of privacy, and it\ncan lead to worse.\nA fundamental aspect of this is that one does not\nknow, indeed cannot know, how data will be used in\nthe future, or what other data they will be linked with.\nThis means we cannot usefully characterize data sets\nas public (vs. not public) or by potential use (since\nthese are unlimited and unforeseeable), and that the in-\ntrinsic nature of the data cannot be used as an argument\nthat they are not risky. It is not the dataper sethat raise\nethical issues, but the use to which they are put and the\nanalysis to which they are subjected.\nIn summary, various properties of modern data and\nthe use of such data make data technology distinct from\nother advanced technologies, requiring careful consid-\neration of ethical issues. These include the following:\nDATA ETHICS IN A CHANGING WORLD 177", "to lead to patient reidentification and subsequent harm and \ndamages.\nAnother example of how minor leaks are often presented \nas major problems can be seen in the following paper. \nSlokom et al. claim synthetic data is not privacy preserving \nbecause they devised an attack which revealed sensitive data \n(Slokom et al. 2022). However, they overlook key contextual \naspects. Most notably, that the leaked sensitive information \nis limited and often does not meaningfully improve upon \nbaseline prediction of sensitive attributes; in some scenarios, \nit even performs worse. Even when successful, the attack \nonly slightly exceeds random performance, achieving about \n60% accuracy on a sensitive binary attribute. This high level \nof uncertainty means that such an attack cannot be deemed \na serious privacy threat in this scenario. If attacks with such \nhigh levels of uncertainty are deemed major breaches of pri-\nvacy, publishing any analysis would be impossible, as even", "suppression becomes increasingly more complex as an or-\nganization wants to make more use of its confidential data. \nIf a suppressed value appears in two tables, then each of \nthose tables requires their own suppression patterns. The \ntwo tables might interact with each other, however, re -\nquiring even more cells to be suppressed. What’s worse, \nif one value of the suppressed cells can be learned from \nanother data source, then it might be possible for some -\none to undo all the confidentiality protections. That’s the \nmosaic effect again.\nWith the birth of the commercial internet in the \n1990s, it suddenly became much easier to distribute data. \nDownloaded from http://direct.mit.edu/books/book-pdf/2509392/book_9780262382168.pdf by guest on 04 November 2025"]}
{"query": "What is GDPR and how does it impact data science projects?", "relevant_passages": ["Data Privacy and Ethical Consideration in Data Science \n  Published By: Fifth Dimension Research Publication                   https://fdrpjournals.org/                                         142 | P a g e  \n \n \n General Data Protection Regulation (GDPR): Enacted by the European Union i n 2018, GDPR is one of the most \ncomprehensive and strict data protection laws globally. It applies to any organization that collects or processes data related to EU \ncitizens, even if the organization is located outside the EU. GDPR emphasizes the importance of user consent, data minimization, \nand the right of individuals to control their personal data. Key principles include: \n \n• Consent: Organizations must obtain clear and informed consent before collecting personal data. \n• Right to Access and Erasure: Individuals have the right to request access to their data and to have it deleted under certain \nconditions (right to be forgotten).", "regulations and, within the United States (US), state-to-state regulations. \nThe most famous piece of law, mentioned above, is the European \nUnion’s General Data Protection Regulation (GDPR). It offers signiﬁ-\ncant protections for data subjects, including rights to transparency, access, \nrectiﬁcation, erasure, and even the right to object to certain forms of \nprocessing. Outside of compliance with the rights of data subjects, the \nGDPR also requires data “controllers” and “processors” to protect their \ndata, pseudonymize it, keep records of processing, and even to appoint \nData Protection Ofﬁcers whose job is to ensure compliance with GDPR \ninstructions. \nFrom the perspective of privacy, the rights of data subjects in the \nGDPR cover plenty of ground. Many understand privacy to concern a", "preceding the other. This provides a baseline for future research into the more \nprecise timing of public pressures, media coverage, and the development of company \npolicies. \n5.2 Anticipate Emerging Regulation \nCompanies generally put resources into curbing externalities because laws require \nthem to do so. At the time of the data collection for this book (2017–2019), US \nlaw did not directly require organizations to weigh the beneﬁts and risks that their \nuse of advanced analytics and AI could create. American companies that processed \nEuropean’s personal data did have to comply with Article 22 of the European \nUnion’s General Data Protection Regulation (GDPR) which expressly regulates", "also recognised in certain legislations. The GDPR (Art. 1(3)), for instance, notes that “the free movement \nof personal data should not be restricted nor prohibited for reasons connected with the protection of natural \npersons with regards to the processing of personal data” (European Union, 2016[43]).  \nSynergies appear high between privacy and AI communities in discussing how different collective interests, \nsuch as better public health or security resulting from AI, can be balanced against increased interference \nstemming from AI systems with certain human rights. A prime example of this pathway is the collective \neffort to leverage data -driven tools, including AI, in the fight against the COVID -19 pandemic while \nrespecting data protection and privacy principles.  \nPrinciple 1.1 also includes harms to vulnerable populations. Economic displacement is not a direct focus \nof data protection law and the effect of AI on jobs is outside the direct scope of data protection regulation.", "to make decisions about treatments and doses for\nindividuals.\n21\nRecital (33) of the GDPR makes an attempt to tackle\nthis intrinsic unknowability of future use: ‘‘It is often\nnot possible to fully identify the purpose of personal\ndata processing for scientiﬁc research purposes at the\ntime of data collection. Therefore, data subjects should\nbe allowed to give their consent to certain areas of sci-\nentiﬁc research when in keeping with recognised ethi-\ncal standards for scientiﬁc research. Data subjects\nshould have the opportunity to give their consent\nonly to certain areas of research or parts of research\nprojects to the extent allowed by the intended purpose.’’\nDATA ETHICS IN A CHANGING WORLD 183"]}
{"query": "How do you ensure that AI models are not discriminatory?", "relevant_passages": ["collaboration to mitigate risks associated with machine learning applications. \n6.1 Bias-Detection Frameworks in AI Models \nAlgorithmic bias remains a s ignificant challenge in ethical AI, as models often inherit systemic biases present in training data [5]. \nTo counteract this, AI systems should incorporate fairness-aware techniques, including: \n• Adversarial Debiasing: Training AI models with adversarial networks to minimize discriminatory patterns while maintaining \npredictive accuracy [1]. \n• Fairness Constraints: Algorithmic measures such as Equalized Odds and Demographic Parity enforce fairness by ensuring \nequitable treatment of different demographic groups [6]. \n• Counterfactual Fairness Testing:  Evaluating AI predictions under \"what -if\" scenarios to detect the influence of sensitive \nattributes (e.g., gender, race) [2]. \nRecent advancements in Explainable AI (XAI) have further enhanced bias detection, allowing researchers to audit and rectify", "To systematically identify and quantify bias, we implement the Fairness-Aware Bias Detection Algorithm  (Algorithm 1). This \nalgorithm takes a trained model and evaluates it on test data while considering a sensitive attribute. It computes the rates of positive \noutcomes for different demographic groups and measures discrepancies in prediction fairness. The results guide interventions to \nimprove fairness and minimize discriminatory effects. \n \nAlgorithm 1 Fairness-Aware Bias Detection Algorithm \n \n1: Input: Trained model, test dataset, sensitive attribute \n2: Output: Bias metrics (Disparate Impact, Equalized Odds) \n3: Generate predictions using the trained model \n4: Compute the proportion of positive predictions for different demographic groups  \n5: Calculate the ratio of positive outcomes (Disparate Impact) \n6: Compare true positive and false positive rates across groups (Equalized Odds)  \n7: Return bias metrics for analysis \n \n4.3 Explainable AI (XAI) for Model Transparency", "International Journal of Current Science Research and Review \nISSN: 2581-8341    \nVolume 08 Issue 03 March 2025    \nDOI: 10.47191/ijcsrr/V8-i3-09, Impact Factor: 8.048   \nIJCSRR @ 2025   \n \nwww.ijcsrr.org \n   \n1071   *Corresponding Author: Chiranjeevi Bura                                                      Volume 08 Issue 03 March 2025 \n                   Available at: www.ijcsrr.org \n                                                              Page No. 1067-1078 \n \n• Counterfactual Fairness (CF) : A model is deemed fair if changing a sensitive attribute (such as gender or ethnicity) does \nnot alter its predictions. This concept helps in assessing whether an AI system makes decisions independently of protected \nattributes. \n4.2 Bias Detection Algorithm \nTo systematically identify and quantify bias, we implement the Fairness-Aware Bias Detection Algorithm  (Algorithm 1). This", "Consisting of six sections, and running almost six pages, the checklist is quite long. \nBut it becomes easier to comprehend when one realizes that it contains several core \nthemes that are repeated throughout the various stages. These are:\n• Identify those whom the AI system in question might impact, including particular \ndemographic groups;\n• Examine the types of fairness-related harms that the AI system might impose on \nsuch stakeholders (e.g., allocation, quality of service, stereotyping, denigration, \nover- or underrepresentation), how these compare to the system’s beneﬁts, and \nwhether there are trade-offs between particular fairness criteria.\n• Scrutinize and clarify deﬁnitions—of system architecture, datasets, potential \nfairness-related harms, fairness criteria and metrics—and revise them as necessary \nto mitigate any fairness-related harms.\n• Solicit input from a diverse group of reviewers and stakeholders regarding vision,", "The above case studies underscore the real-world ethical risks associated with AI in hiring, finance, law enforcement, and privacy. \nAI biases can exacerbate economic inequality, racial discrimination, and privacy violations, making it critical to adopt a multifaceted \napproach to AI ethics. Addressing these dilemmas requires: \n• Algorithmic Fairness Techniques: Developing bias-aware machine learning models. \n• Regulatory Enforcement: Strengthening compliance with AI fairness and transparency laws. \n• Explainability and Audits: Implementing XAI techniques to improve AI accountability.  \nBy integrating responsible AI design principles, policymakers and engineers can mitigate algorithmic harms and e nsure AI-driven \ndecision-making prioritizes fairness and societal well-being."]}
{"query": "What is algorithmic accountability?", "relevant_passages": ["3.2 Accountability:  \n  The issue of accountability is also central to the ethics of data science. Pasquale (2015) has raised concerns about the \nopacity of many machine learning models, particularly deep learning systems, which are often seen as \"black boxes\" due to their \ncomplexity. This lack of transparency makes it difficult to understand how decisions are made and who should be held accountable \nwhen things go wrong.  As a result, scholars have called for greater efforts to make algorithms interpretable and to ensure that  \ndata scientists and organizations can be held accountable for the outcomes of their models (Doshi-Velez & Kim, 2017). \n \n3.3 Transparency:  \n  Transparency, closely related to  accountability, refers to the need for clear communication about how data is collected, \nprocessed, and used. Studies by Diakopoulos (2016)  and Mittelstadt et al. (2016) emphasize that individuals should have a right", "how certain outcomes are derived. This \"black box\" issue undermines trust in these systems. To counter this, responsible data  \nscience emphasizes the need for model interpretability and transparency, providing s takeholders with clear explanations of how \nalgorithms make decisions and the data used in the process. \n \n4.2 Accountability \n  Accountability in data science implies that organizations and individuals responsible for data collection, processing, and \nanalysis must be answerable for the outcomes of their models. This means being proactive in identifying and addressing risks \nassociated with data misuse, bias, or unethical application of algorithms. Ethical oversight mechanisms, such as internal audits or \nexternal reviews, help ensure that data science practices align with organizational values and societal expectations.  \n \nV.REGULATION AND GOVERNANCE \nAs data science becomes more deeply embedded in society, the regulation and governance of data collection, processing, and", "ensure fairness and accountability. Algorithmic audits examine the data inputs, model design, and outcomes to detect biases, \nunfair treatment, or unintended  consequences that may arise from data-driven decisions. This ensures that data science \napplications, especially those involving AI, operate transparently and ethically.", "ments as central elements of the governance of automated\ndecision systems in Europe and the United States. They look\nat commonalities and differences between the Algorithmic\nAccountability Act of 2022 in the US congress, the New York\nCity’s Int. 1894 from 2021, California’s Assembly Bill 13 (AB\n13) from 2021, and European Union AI Act from 2021. The\nauthors conclude that there is a trend toward impact assess-\nments in the regulation of algorithmic systems, which could\ncreate ‘‘shared ground truths’’ that could enable other forms of\naccountability. They urge developers and deployers of auto-\nmated decision-making systems to prepare for this new reality\nand ensure they are able to carry out such assessments.\nFinally, Di Cara et al. explore how community-based collabo-\nrative spaces can contribute to foster quality data science work\nthat is in line with data ethics. They illustrate this effort by\nexample of the Data Ethics Club that they created as a regular", "enforcement, and I’ve impressed this on the engineers a number of times, you’re \npointing a loaded gun at someone basically. Are we 100% conﬁdent in the analysis \nthat we’re supporting here, and if we’re not, then the consequences are that level of \nseriousness.” (Interviewee #10). \n3.6 Opacity and Procedural Unfairness \nWhile it is true that algorithmic decision-makers can make errors, the same can be \nsaid for human decision-makers. The key distinction between algorithmic and human \ndecision-making is not the former’s capacity for error, but rather its opacity and \nimperviousness to challenge. For example, where a company determines through \nadvanced analytics that an employee would not succeed in a higher position and \ndenies the person a promotion, the employee would have no way to know what \ndata or algorithm had resulted in this determination, and no way to challenge them \n(Rubinstein \n2013). Such algorithmic determinations are a “black box” as far as the"]}
{"query": "When is it unethical to use synthetic data?", "relevant_passages": ["conce Pts  and theor Ies   107\npart). This means that some respondent’s answers count \nmore than others, which makes computing the sensitiv -\nity much harder. In 2020, such computations were beyond \nDP’s state of the art.\nValidation and Verification Servers\nOne of the frustrations—  and f ears— of u sing synthetic \ndata is the knowledge that a scientific discovery made with \nsuch data might not be real. Instead, the discovery might \njust be finding an artifact from the process that created \nthe synthetic data. This isn’t a risk just for differential pri-\nvacy: it has the potential of happening any time that data \nare edited to preserve privacy and then used for scientific \nresearch. As researchers Arthur Kennickell and Julia Lane \nobserved in 2006,\nDespite the fact that much empirical economic \nresearch is based on public- us e data files, the \ndebate on the impact of disclosure protection on \ndata quality has largely been conducted among", "to lead to patient reidentification and subsequent harm and \ndamages.\nAnother example of how minor leaks are often presented \nas major problems can be seen in the following paper. \nSlokom et al. claim synthetic data is not privacy preserving \nbecause they devised an attack which revealed sensitive data \n(Slokom et al. 2022). However, they overlook key contextual \naspects. Most notably, that the leaked sensitive information \nis limited and often does not meaningfully improve upon \nbaseline prediction of sensitive attributes; in some scenarios, \nit even performs worse. Even when successful, the attack \nonly slightly exceeds random performance, achieving about \n60% accuracy on a sensitive binary attribute. This high level \nof uncertainty means that such an attack cannot be deemed \na serious privacy threat in this scenario. If attacks with such \nhigh levels of uncertainty are deemed major breaches of pri-\nvacy, publishing any analysis would be impossible, as even", "spective and understanding of what kind of ethical behavior is expected and how the\ndata can be handled (O ’Leary, 2016).\nAccording to Steinmann et al. ( 2016), big data analysis presents two main ethical\nconcerns: the fact that big data tend to deal with populations rather than individual\nsamples and that big data can be reused, repurposed, recombined, and reanalyzed\n(4R challenge). One speaks of reusing when the data is used for other purposes\nwithin the same domain in addition to what it was initially collected for (Steinmann\net al., 2016). Based on the misconception that publicly available data does not\nimpose any further harm, Metcalf and Crawford ( 2016) state that this ethical risk\nrelating to individuals and communities is continuously overlooked in the ﬁeld of\nbig data. Here, it is signi ﬁcant to review the conditions of the initial dataset and to\nargue who, in such a case, is genuinely responsible for the various outputs (Leonelli,\n2016).", "ﬁed by linking public domain anonymized data sets. At\nthe very least, this involves invasion of privacy, and it\ncan lead to worse.\nA fundamental aspect of this is that one does not\nknow, indeed cannot know, how data will be used in\nthe future, or what other data they will be linked with.\nThis means we cannot usefully characterize data sets\nas public (vs. not public) or by potential use (since\nthese are unlimited and unforeseeable), and that the in-\ntrinsic nature of the data cannot be used as an argument\nthat they are not risky. It is not the dataper sethat raise\nethical issues, but the use to which they are put and the\nanalysis to which they are subjected.\nIn summary, various properties of modern data and\nthe use of such data make data technology distinct from\nother advanced technologies, requiring careful consid-\neration of ethical issues. These include the following:\nDATA ETHICS IN A CHANGING WORLD 177"]}
{"query": "How can data poisoning attacks affect ML models?", "relevant_passages": ["can conflict with the data minimisation principle (GPA, 2023[15]). Advances in machine learning that require \nless data, or that process data in protected ways – such as Privacy Enhancing Technologies (PETs) – can \nhelp reduce the gap between the development of safe AI models and the protection of individuals’ privacy \nrights. However, more analysis is needed to understand how such considerations can be handled.  \nWhile traditional cyberattacks are enabled by vulnerabilities in software programs, their configuration and \ninterface, attacks on AI systems are facilitated by inherent limitations in the underlying AI algorithms  \n(Comiter, 2019[76]). Manipulating the input fed into the AI system to alter the output or corrupting the AI \nmodel itself, making it inherently flawed, are  some examples (Comiter, 2019 [76]). At the same time, \nempirical studies (Liwei Song, 2019[77]) show that privacy and security can sometimes be in tension: when", "114   cha Pter 1\nattacker turns the model upside down and shakes it until \nthe training data fall out.\nWhen differential privacy is applied to machine learn-\ning, statistically controlled noise is injected into the data \npipeline as part of the training process. Such approaches, \nmany of them based on Google’s “stochastic gradient de-\nscent with differentially private updates,” can limit the \nprobability of extracting training data from a classifier. 29 \nRecently, DP has been applied to large language models, \nwhere it can similarly decrease the possibility of extracting \nsensitive training data.30\nProtecting machine learning models from attack is es-\npecially important when these models are deployed onto \nconsumer smartphones or video cameras. Since these de-\nvices are physically under the control of end users, it can \nbe hard, if not impossible, to prevent capable malicious \nusers from getting direct access to models and attempting", "3.2 Data Validity\nErrors in data analysis may not only lead to a lack of validity but may also result in\nethically problematic results with far-reaching consequences. As data form the\nfoundation for decisions and indicate options for action, any errors that occur during\nthe data collection, input, or processing steps can prompt results in the wrong\ndirection (Lever et al., 2016). On the one hand, the results may appear incompre-\nhensible or dif ﬁcult to interpret, contributing to false conclusions. Worse still, they\ncan have fatal consequences for the individual as well (Balas et al., 2015). For\nexample, Amazon faced massive problems with an AI solution for their internal\nrecruiting due to using a machine learning algorithm that was biased towards\nwomen. The historical training data, which served as input for the algorithm, was\ndistorted by the male-dominated working environment of the technical world, thus\ndiscriminating against women (Vincent, 2018). Along similar lines, women were"]}
{"query": "What are dark patterns in user data collection?", "relevant_passages": ["studies brought up is that of organizations with \nindividuals. This was evidenced in the Tech Giant Inc. \ncase, where most large tech companies full y control \nuser data. This often leads to unhealthy \nauthoritarianism where the data controller can engage \nin distinctly unethical ventures , such as spying to \ngather more data, which is then sold for profit. \n \nSurveillance, especially i n Workplace Performance \nAnalytics, is a major concern in how organizations", "process, and utilize personal information for activities such as targeted \nadvertisements and tailored user experiences. While these activities can \nhelp improve situational awareness and increase overall user experiences, \nusers should take an active role in how their personal data is collected and \nused. \nReferences \nDalenius, T. (1986). Finding a Needle in a Haystack or Identifying Anonymous \nCensus Records. Journal of Ofﬁcial Statistics, 2(3), 329–336. \nHsu, J. (2018). The Strava Heat Map and the End of Secrets, Wired. Retrieved \nAugust 22, 2023, from https://www.wired.com/story/strava-heat-map-mil \nitary-bases-ﬁtness-trackers-privacy/"]}
{"query": "Why is explainability important in high-stakes ML systems?", "relevant_passages": ["explained their results via decision rule (Mitchell et al. 1986). The current emphasis \non explainability arises largely from the fact that contemporary models are much \nmore complex (some with several billion parameters) and are increasingly used \nto make critical decisions. Justifying such decisions in the presence of increasing \nregulatory requirements ampliﬁes the need for explainability in such models (Biran \nand Cotton \n2017). \nSeveral of the interviewees emphasized the importance of explainability. One \nexplained that explainability is key to ethical data governance. One pointed out that \nexplainability is critical to trust. (Interviewee #9). Another described how difﬁcult it \ncan be to explain models and how the company arrived at them: \nWhen we get into AI and machine learning, sometimes it’s pretty challenging to describe, \nto understand what transparency means. In the old days you could say, we take an email", "9.3 The Clear and Pressing Need for Explainable Algorithms 87\n9.3 The Clear and Pressing Need for Explainable \nAlgorithms \nSomewhat related to, and yet distinct from, the notion of fair algorithms is the ability \nto understand what these complex ML and AI algorithms are exactly doing under \nthe hood. (Samek et al. 2019; Gunning and Aha 2019). Explainability and model \ntransparency facilitates trust and ensures regulatory policies are being met. \nExplainability in data analysis and AI is not a new topic. A famous early example \nwas during the 1854 outbreak of Cholera in London where Dr. John Snow presented \nauthorities with a simple cluster map showing that the disease spread clustered around \na water pump on Broad Street in London convincing them to take action (Tulchinsky \n2018). Explainability in AI dates back at least four decades to expert systems that \nexplained their results via decision rule (Mitchell et al. 1986). The current emphasis", "the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) [8]. \nTransparency and accountability in AI decision-making are also growing concerns. Many machine learning models operate as “black \nboxes,” making it difficult to interpret how decisions are made, particularly in high-stakes applications like medical diagnostics and \nfinancial risk assessments [9]. A lack of interpretability in AI systems can e rode trust and hinder regulatory oversight. Explainable \nAI (XAI) techniques, such as Shapley Additive Explanations (SHAP) and Local Interpretable Model -Agnostic Explanations \n(LIME), are being increasingly adopted to address these concerns by enhancing AI t ransparency and interpretability [10].", "advocate for AI systems that uphold non-discrimination, privacy protection, and ethical AI governance [17]. \nExplainable AI (XAI) and Algorithmic Transparency:  The XAI movement seeks to develop machine learning models that are \ninterpretable and explainable to non-expert users. Ensuring transparency in AI decisions is crucial in domains such as credit scoring, \nhiring, and criminal justice, where algorithmic decisions significantly impact individu -als [8]. Studies show that explainable AI"]}
{"query": "What safeguards can be used when deploying AI in healthcare?", "relevant_passages": ["Ethics of AI in Healthcare: In healthcare applications, AI ethics frameworks prioritize fairness in clinical decision-making, patient \nconsent, and algorithmic accountability. The World Health Organization (WHO) and other regulatory bodies emphasize human-in-\nthe-loop AI design to prevent ethical failures in automated medical diagnostics [18]. Ethical concerns in AI -driven digital \nphenotyping and patient monitoring underscore the necessity for data security, explainability, and privacy protection [6]. \n3.4 Emerging Ethical Frameworks in AI Research \nAI Ethics and Human Rights: Ethical AI research has expanded to include human rights-based approaches, ensuring that AI does \nnot reinforce systemic discrimination, surveillance risks, or algorithmic oppression [14]. Institutions such as the United Nations \nadvocate for AI systems that uphold non-discrimination, privacy protection, and ethical AI governance [17].", "giving them new relevance. In this regard, “ethical issues arise in terms of ownership\nof data, how data are used, and how the privacy of those from whom the data is\nderived is protected ” (Brady & Neri, 2020, p. 232). Here, the importance of\nanonymizing data arises as patient re-identi ﬁcation can bring about unwanted\nadvertising or even lead to medical records being brought forward to the public\n(Brady & Neri, 2020). Other ethical issues include topics such as resource inequality,\nliability, conﬂicts of interest, and workforce disruption. Overall, a framework for AI,\nnot only within the health and medical sectors, should be deemed necessary in order\nto protect human rights, foster user safety, discuss the roles of future diagnosticians\nand medical specialists, and raise awareness of the risks of AI tools (Benke & Benke,\n2018; Brady & Neri, 2020).\nIn addition, AI faces two principal challenges: technical and normative. The", "Mitigations (NIST, 2024[80]).  \nDeception arising from generative AI models can also result in security vulnerabilities, especially when the \nAI system is employed in specific contexts, such as law enforcement, medicine, education, or employment. \nGovernments have recently started to explore the use of new tools to identify and manage t he security \nrisks associated with generative AI. For example,  the 2023 US Executive Order on the Safe, Secure, and \nTrustworthy Development and Use of Artificial Intelligence, emphasises the importance of “red teaming” – \na process of identifying security ri sks through groups of individuals attempting to subvert security \nsafeguards as a means to test AI systems - as a helpful testing process to identify vulnerabilities and flaws \nin AI systems (The White House, 2023[4]).  \n \nKey policy consideration: Principle 1.4 - Robustness, security and safety \nThere exists considerable alignment, and potential synergies, between AI and privacy policy", "Of the dangers , I find two of them  quite prominent . \nThe first is called algorithmic bias, where models are \ndesigned from biased datasets and, in return, will \ngive biased results. According to Johnson et al. (2022), \nmore than 40% of the predictive analytics models \nimplemented in healthcare facilities had unequal \noutcomes with negative effects o n minorities. As \nstated earlier, there is a need for better governance o f \nAI-based decision-making. \n \nIn addition, existing policies do not provide adequate \nprotection against the abuse of data by entities that \npossess tremendous amounts of information while \nrespecting the rights of data subjects. There is concern \nand a request to announce more concrete regulatory \nframeworks for technologies and their ethical impacts \nbased on new features, including artificial intelligence \nand machine learning. \n \nIII. METHODOLOGY \n \n3.1 Research Design \nThis research uses qualitative and quantitative", "disparities in accuracy across racial and gender groups, raising concerns about their deployment in criminal justice applicat ions [2, \n3]. Without proper bias mitigation strategies, AI systems risk perpetuating and even amplifying systemic discrimination.  \nAnother critical ethical issue is privacy infringement. Many AI models rely on extensive datasets containing sensitive person al \ninformation, making them vulnerable to data breaches, unauthorized access, and mass surveillance [4]. Behavioral analytics in \ngovernment AI deployments have raised alarms about mass surveillance and potential violations of individual freedoms [5]. In \nhealthcare, AI -driven p redictive models process vast amounts of patient data, bringing challenges in data security, patient \nconfidentiality, and informed consent [6, 7]. Ensuring ethical AI deployment requires compliance with privacy regulations such as \nthe General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) [8]."]}
{"query": "How do you handle user data minimization in a production system?", "relevant_passages": ["insights without revealing information about individual data points. This technique ensures that the inclusion or exclusion of any \nsingle data point does not significantly affect the outcome of an analysis, making it difficult to infer specific information about \nindividuals. \nFor example, companies like Apple and Google have implemented differential privacy in their systems to analyze user behavior \nwhile safeguarding personal information. Differential privacy is increasingly being adopted by both private organizations and \ngovernmental institutions to balance the need for data analysis with individual privacy. \n \n6.3 Data Minimization and Purpose Limitation \n  Data minimization is a privacy principle that mandates organizations to collect only the data that is necessary for a specific", "privacy community. \nPrivacy laws also provide that only the minimum amount of personal data necessary should be processed \nto achieve the intended purpose. This so-called data minimisation principle is implicit in the OECD Privacy \nGuidelines and made explicit in various privacy laws, such as the GDPR or the  California Privacy Rights \nAct. AI business models , especially in the wake of generative AI,  have followed the assumption that \ncollecting extensive amounts of data is essential for the effective operation of AI systems, especially during \nthe training phase . This approach may conflict with the data minimisation principle, since it may not be \npossible to map in advance what personal data the AI system requires.  \nYet the concept of data minimisation does not mean either to completely avoid processing personal data \nor limiting the amount of data to a specific volume  (ICO, 2023[59]). The application of this principle should", "References 89\nWhen we talk to the data lake team or the data governance team, this is a point that we make \nabsolutely clear to them, that at no point would this data transfer or be pulled from the data \nlake environment or from a secondary source from data lake and back into any of the credit \nsystems . . . . There are administrative and physical controls… different users are basically \nconﬁned to play in their space. (Interviewee #17). \nAnother talked about the use of “virtualization... virtually representing data \nwithout actually using it or copying it” as a way to “represent all data across an \norganization in one single place” and so provide an effective access control mecha-\nnism. (Interviewee #5) (Singh et al. 2008; Soror et al. 2007). Virtualization can also \nassist in addressing the “reproducibility crisis” in machine learning 2 —the situation \nin which data scientists often cannot reproduce results across teams. (Interviewee"]}
{"query": "What is an agentic AI system and how does it differ from typical ML models?", "relevant_passages": ["## 8. Conclusion\n      The emergence of AI agents is undeniably reshaping the workplace landscape in 5. With their ability to automate tasks, enhance efficiency, and improve decision-making, AI agents are critical in driving operational success. Organizations must embrace and adapt to AI developments to thrive in an increasingly digital business environment.\n      ```\n    </CodeGroup>\n  </Step>\n</Steps>\n\n<Check>\n  Congratulations!\n\n  You have successfully set up your crew project and are ready to start building your own agentic workflows!\n</Check>\n\n### Note on Consistency in Naming\n\nThe names you use in your YAML files (`agents.yaml` and `tasks.yaml`) should match the method names in your Python code.\nFor example, you can reference the agent for specific tasks from `tasks.yaml` file.\nThis naming consistency allows CrewAI to automatically link your configurations with your code; otherwise, your task won't recognize the reference properly.\n\n#### Example References", "- **Task Automation**: AI agents can carry out repetitive tasks such as data entry, scheduling, and payroll processing without human intervention, greatly reducing the time and resources spent on these activities.\n      - **Improved Efficiency**: By quickly processing large datasets and performing analyses that would take humans significantly longer, AI agents enhance operational efficiency. This allows teams to focus on strategic tasks that require higher-level thinking.\n      - **Enhanced Decision-Making**: AI agents can analyze trends and patterns in data, provide insights, and even suggest actions, helping stakeholders make informed decisions based on factual data rather than intuition alone.\n\n      ## 3. Popular AI Agent Frameworks\n      Several frameworks have emerged to facilitate the development of AI agents, each with its own unique features and capabilities. Some of the most popular frameworks include:", "# Introduction\n\n> Build AI agent teams that work together to tackle complex tasks\n\n# What is CrewAI?\n\n**CrewAI is a lean, lightning-fast Python framework built entirely from scratch—completely independent of LangChain or other agent frameworks.**\n\nCrewAI empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario:\n\n* **[CrewAI Crews](/en/guides/crews/first-crew)**: Optimize for autonomy and collaborative intelligence, enabling you to create AI teams where each agent has specific roles, tools, and goals.\n* **[CrewAI Flows](/en/guides/flows/first-flow)**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively.\n\nWith over 100,000 developers certified through our community courses, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.\n\n## How Crews Work"]}
{"query": "What are “tools” in agentic frameworks and how do agents use them?", "relevant_passages": ["### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/python/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```python wrap theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n```", "### How It All Works Together\n\n1. The **Crew** organizes the overall operation\n2. **AI Agents** work on their specialized tasks\n3. The **Process** ensures smooth collaboration\n4. **Tasks** get completed to achieve the goal\n\n## Key Features\n\n<CardGroup cols={2}>\n  <Card title=\"Role-Based Agents\" icon=\"users\">\n    Create specialized agents with defined roles, expertise, and goals - from researchers to analysts to writers\n  </Card>\n\n  <Card title=\"Flexible Tools\" icon=\"screwdriver-wrench\">\n    Equip agents with custom tools and APIs to interact with external services and data sources\n  </Card>\n\n  <Card title=\"Intelligent Collaboration\" icon=\"people-arrows\">\n    Agents work together, sharing insights and coordinating tasks to achieve complex objectives\n  </Card>\n\n  <Card title=\"Task Management\" icon=\"list-check\">\n    Define sequential or parallel workflows, with agents automatically handling task dependencies\n  </Card>\n</CardGroup>\n\n## How Flows Work", "***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.\n</Tip>\n# Agents\n\nAgents combine language models with [tools](/oss/python/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached."]}
{"query": "Describe the concept of goal specification in autonomous agents.", "relevant_passages": ["<Tip>\n  To learn more about tools, see [Tools](/oss/python/langchain/tools).\n</Tip>\n\n### System prompt\n\nYou can shape how your agent approaches tasks by providing a prompt. The [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)) parameter can be provided as a string:\n\n```python wrap theme={null}\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n```\n\nWhen no [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\\(system_prompt\\)) is provided, the agent will infer its task from the messages directly.\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/python/langchain/middleware).", "### How It All Works Together\n\n1. The **Crew** organizes the overall operation\n2. **AI Agents** work on their specialized tasks\n3. The **Process** ensures smooth collaboration\n4. **Tasks** get completed to achieve the goal\n\n## Key Features\n\n<CardGroup cols={2}>\n  <Card title=\"Role-Based Agents\" icon=\"users\">\n    Create specialized agents with defined roles, expertise, and goals - from researchers to analysts to writers\n  </Card>\n\n  <Card title=\"Flexible Tools\" icon=\"screwdriver-wrench\">\n    Equip agents with custom tools and APIs to interact with external services and data sources\n  </Card>\n\n  <Card title=\"Intelligent Collaboration\" icon=\"people-arrows\">\n    Agents work together, sharing insights and coordinating tasks to achieve complex objectives\n  </Card>\n\n  <Card title=\"Task Management\" icon=\"list-check\">\n    Define sequential or parallel workflows, with agents automatically handling task dependencies\n  </Card>\n</CardGroup>\n\n## How Flows Work", "| Component     |         Description        | Key Features                                                                                                                      |\n| :------------ | :------------------------: | :-------------------------------------------------------------------------------------------------------------------------------- |\n| **Crew**      | The top-level organization | • Manages AI agent teams<br />• Oversees workflows<br />• Ensures collaboration<br />• Delivers outcomes                          |\n| **AI Agents** |  Specialized team members  | • Have specific roles (researcher, writer)<br />• Use designated tools<br />• Can delegate tasks<br />• Make autonomous decisions |\n| **Process**   | Workflow management system | • Defines collaboration patterns<br />• Controls task assignments<br />• Manages interactions<br />• Ensures efficient execution  |"]}
{"query": "How can agents safely access external APIs?", "relevant_passages": ["to third-party developers of AI systems who may occupy the role of data intermediaries; iv) guidance on \nbest practices to support businesses in their compliance with the Personal Data Protection Act (PDPC, \n2022[97]).", "dI fferent Ial  Pr Iv acy Issues   159\nIt is also possible to create a data enclave entirely \nwithin advanced microprocessors in a special secure pro-\ncessing area called a trusted execution environment (TEE). \nIn these cases, the data are made available to researchers \nin an encrypted form that can only be decrypted inside the \nmicroprocessor by a specific program that has been vet -\nted and approved by the data owner. The program typically \nperforms some kind of statistical manipulation, but will \nnot allow the decrypted confidential data to be exported \nfrom the data enclave.\nPolicy Shortcomings\nIn addition to the technical limitations, there are many \npolicy shortcomings that limit the deployment of differ -\nential privacy.\nPeople Implementing Privacy Policies Lack Tools and \nExperience for Setting the PLBs\nOne of the core principles of DP is that policy should de-\ncide the trade- off b etween accuracy and privacy loss. But", "Indeed, online consent is often obtained through pop-up, \nsometimes pre-ticked consent boxes to websites and apps \nthat collect personal information (Jablonowska and Micha-\ntowicz 2020). Furthermore, as discussed earlier in this arti-\ncle, “pay or okay” subscription models have been devel-\noped by companies such as Meta to give users the illusion \nof agency—although this practice was deemed incompat-\nible with providing a genuine and free choice for users by \nthe EDPB (European Data Protection Board 2024 ). Still, \nsome controllers may attempt to invoke legitimate interest \nto do away with consent requirements for scraping per -\nsonal data for AI model training. Finally, in practice, users \noften have limited visibility over whether their information \nwas used to train AI models, how to opt out of this pro -\ncess (Grinkevičius 2024; Mauran 2024; Fitzgerald 2024; \nHoe Meta Gegevens Gebruikt 2024; Shah 2024), and it is \nthus difficult for them to become aware of malpractice and"]}
{"query": "What is the role of memory in agentic architectures?", "relevant_passages": ["```python  theme={null}\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n<Note>\n  As of `langchain 1.0`, custom state schemas **must** be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the [v1 migration guide](/oss/python/migrate/langchain-v1#state-type-restrictions) for more details.\n</Note>\n\n<Tip>\n  To learn more about memory, see [Memory](/oss/python/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/python/langchain/long-term-memory).\n</Tip>\n\n### Streaming", "```python  theme={null}\n    from dataclasses import dataclass\n\n    # We use a dataclass here, but Pydantic models are also supported.\n    @dataclass\n    class ResponseFormat:\n        \"\"\"Response schema for the agent.\"\"\"\n        # A punny response (always required)\n        punny_response: str\n        # Any interesting information about the weather if available\n        weather_conditions: str | None = None\n    ```\n  </Step>\n\n  <Step title=\"Add memory\">\n    Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n    the agent to remember previous conversations and context.\n\n    ```python  theme={null}\n    from langgraph.checkpoint.memory import InMemorySaver\n\n    checkpointer = InMemorySaver()\n    ```\n\n    <Info>\n      In production, use a persistent checkpointer that saves to a database.\n      See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.\n    </Info>\n  </Step>"]}
{"query": "How do agents evaluate whether their actions were successful?", "relevant_passages": ["88 9 Technical Solutions\nlarger picture. Our point . . . is risk management and failure mitigation . . . . From a risk \nmanagement perspective, there are a variety of different ways and processes and things we \ncan do to help govern these models even if we don’t explain them . . . . Think about failure. \nWhat does failure mean to you? As a company how would you react? What processes are in \nplace? ....O n e o f t h ed o w n s i d e s o f m o d e l s t h a t a r eh a r dt oe x p l a i ni sw h e nt h e yf a i l , i t ’ s \nhard to understand why. (Interviewee #5). \n9.4 Algorithmic Auditing of Data Use \nThe modern economy is increasingly reliant on our ability to generate and store large \ntracts of data and realize actionable insights from this data. The algorithmic steps \nby which these insights are discerned and subsequently shared are often complex— \nrequiring multiple transformative steps. These complex multi-step processes in turn", "* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic."]}
{"query": "How does tool-use affect the reasoning capabilities of an agent?", "relevant_passages": ["### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n* Multiple tool calls in sequence (triggered by a single prompt)\n* Parallel tool calls when appropriate\n* Dynamic tool selection based on previous results\n* Tool retry logic and error handling\n* State persistence across tool calls\n\nFor more information, see [Tools](/oss/python/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n```python wrap theme={null}\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n```", "### Agentic RAG\n\n**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.\n\n<Tip>\n  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.\n</Tip>\n\n```mermaid  theme={null}\ngraph LR\n    A[User Input / Question] --> B[\"Agent (LLM)\"]\n    B --> C{Need external info?}\n    C -- Yes --> D[\"Search using tool(s)\"]\n    D --> H{Enough to answer?}\n    H -- No --> B\n    H -- Yes --> I[Generate final answer]\n    C -- No --> I\n    I --> J[Return to user]", "***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.\n</Tip>\n# Agents\n\nAgents combine language models with [tools](/oss/python/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides a production-ready agent implementation.\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached."]}
{"query": "What safety risks arise from autonomous multi-step actions?", "relevant_passages": ["requiring multiple transformative steps. These complex multi-step processes in turn \ncan lead to several sources of risk at each step. The ability to mitigate such risk \nrequires the ability to audit the algorithms to ensure that stated ethical data governance \npolicies and regulatory requirements are being met (Raji et al. \n2020). At the time of \nthe interviews, organizations were beginning to examine how best to audit whether \nalgorithms meet predetermined speciﬁcations. \nNext year, we’re actually going to begin auditing. Part of making sure we’re doing what we \nneed to do is to make sure another set of eyes comes in, and we’re going to be opening up \nto audit the request, the conditions, the compliance with those requests, and the assurance \nthe data is being used the way we directed them to use it. (Interviewee #16). \nWhile such efforts date back to the Gedanken-experiments on black box \nautomaton machines by E. F. Moore in 1950s (Moore 1956), research has largely been", "88 9 Technical Solutions\nlarger picture. Our point . . . is risk management and failure mitigation . . . . From a risk \nmanagement perspective, there are a variety of different ways and processes and things we \ncan do to help govern these models even if we don’t explain them . . . . Think about failure. \nWhat does failure mean to you? As a company how would you react? What processes are in \nplace? ....O n e o f t h ed o w n s i d e s o f m o d e l s t h a t a r eh a r dt oe x p l a i ni sw h e nt h e yf a i l , i t ’ s \nhard to understand why. (Interviewee #5). \n9.4 Algorithmic Auditing of Data Use \nThe modern economy is increasingly reliant on our ability to generate and store large \ntracts of data and realize actionable insights from this data. The algorithmic steps \nby which these insights are discerned and subsequently shared are often complex— \nrequiring multiple transformative steps. These complex multi-step processes in turn"]}
{"query": "Describe how agents can self-correct during execution.", "relevant_passages": ["* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.", "class A,J startend\n    class B,C,E,F,I process\n    class D,G,H decision\n```\n\nThis architecture is suitable for:\n\n* Applications with ambiguous or underspecified queries\n* Systems that require validation or quality control steps\n* Workflows involving multiple sources or iterative refinement\n\n<Card title=\"Tutorial: Agentic RAG with Self-Correction\" icon=\"robot\" href=\"/oss/python/langgraph/agentic-rag\" arrow cta=\"Learn more\">\n  An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.\n</Card>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx)\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.\n</Tip>"]}
{"query": "What is a guardrail and how is it enforced in agent systems?", "relevant_passages": ["51\n52\n53\n54\n55\n56\n # This should trip the guardrail\n    \n          Runner.run(agent, \n         ( )\n    except GuardrailTripwireTriggered:\n        ( )\ntry:\nawait\nprint\n print\n\"I think I might cancel my subscription\")\n\"Guardrail didn't trip - this is unexpected\"\n\"Churn detection guardrail tripped\"\n3 0 A  p r a c t i c a l  g u i d e  t o  b u i l d i n g  a g e n t s", "S i n g l e - a g e n t  s y s t e m s\nA  single agen t can handle man y  task s b y  incr emen tally  adding t ools,  k eeping comple xity  \nmanageable and simplifying e v alua tion and main t enance .  E ach ne w  t ool e xpands its capabilities \nwithout pr ema tur ely  f or cing y ou t o or chestr a t e multiple agen ts.\nTools\nGuardrails\nHooks\nInstructions\nAgentInput Output\nE v ery  or chestr a tion appr oach needs the concep t o f  a ‘ run ’ ,  typically  implemen t ed as a loop tha t \nle ts agen ts oper a t e un til an e xit condition is r eached.  Common e xit conditions include t ool calls,   \na certain struc tur ed output,  err or s,  or  r eaching a maximum number  o f  turns.  \n1 4 A  p r a c t i c a l  g u i d e  t o  b u i l d i n g  a g e n t s", "* Process state before the model is called (e.g., message trimming, context injection)\n* Modify or validate the model's response (e.g., guardrails, content filtering)\n* Handle tool execution errors with custom logic\n* Implement dynamic model selection based on state or context\n* Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.", "26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n         ctx: RunContextWrapper , agent: Agent,  | \n[TResponseInputItem]\n) -> GuardrailFunctionOutput:\n    result =  Runner.run(churn_detection_agent, , \ncontext=ctx.context)\n\n      GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_churn_risk,\n    )\n\ncustomer_support_agent = Agent(\n    name=\n    instructions=\n,\n    input_guardrails=[\n        Guardrail(guardrail_function=churn_detection_tripwire),\n    ],\n)\n \n main():\n    \n      Runner.run(customer_support_agent, \"Hello!\")\n  (\"Hello message passed\")\n   \n[None] input: str\nlist\nawait input\nreturn\nasync def\nawait\n   print\n\"Customer support agent\",\n\"You are a customer support agent. You help customers with \ntheir questions.\"\n# This should be ok\n2 9 A  p r a c t i c a l  g u i d e  t o  b u i l d i n g  a g e n t s"]}
{"query": "How do agents coordinate when working in multi-agent systems?", "relevant_passages": ["| Component     |         Description        | Key Features                                                                                                                      |\n| :------------ | :------------------------: | :-------------------------------------------------------------------------------------------------------------------------------- |\n| **Crew**      | The top-level organization | • Manages AI agent teams<br />• Oversees workflows<br />• Ensures collaboration<br />• Delivers outcomes                          |\n| **AI Agents** |  Specialized team members  | • Have specific roles (researcher, writer)<br />• Use designated tools<br />• Can delegate tasks<br />• Make autonomous decisions |\n| **Process**   | Workflow management system | • Defines collaboration patterns<br />• Controls task assignments<br />• Manages interactions<br />• Ensures efficient execution  |", "### How It All Works Together\n\n1. The **Crew** organizes the overall operation\n2. **AI Agents** work on their specialized tasks\n3. The **Process** ensures smooth collaboration\n4. **Tasks** get completed to achieve the goal\n\n## Key Features\n\n<CardGroup cols={2}>\n  <Card title=\"Role-Based Agents\" icon=\"users\">\n    Create specialized agents with defined roles, expertise, and goals - from researchers to analysts to writers\n  </Card>\n\n  <Card title=\"Flexible Tools\" icon=\"screwdriver-wrench\">\n    Equip agents with custom tools and APIs to interact with external services and data sources\n  </Card>\n\n  <Card title=\"Intelligent Collaboration\" icon=\"people-arrows\">\n    Agents work together, sharing insights and coordinating tasks to achieve complex objectives\n  </Card>\n\n  <Card title=\"Task Management\" icon=\"list-check\">\n    Define sequential or parallel workflows, with agents automatically handling task dependencies\n  </Card>\n</CardGroup>\n\n## How Flows Work"]}
{"query": "What metrics would you use to evaluate the performance of an agentic AI?", "relevant_passages": ["• Error or precision values according to the specific metrics used to measure the validity of the \ninference. \n• Existence or non-existence of qualified human supervision. \n• References to audits, especially audits on possible deviations of infe rence results, as well as the \ncertification or certifications of the [AI] system. For adaptive or evolutionary systems, the last audit \nconducted.   \n• If the [AI] system includes information referring to identifiable third parties, the prohibition of \nprocessing such information without legitimization and the consequences of doing so.    \nAt the same time, it is worth noting that some PEAs  (e.g. the UK ICO), have issued detailed guidance on \nexplainability, and emphasised the importance of considering the audienc e for explanations beyond just \nthe affected individual. This includes staff members who rely on the AI system to make decisions and need", "of the algorithm may be obscure, confusing or lead to information fatigue. Sufficient information should be \nprovided in an accessible manner  to enable the subjects to understand the behaviour of the processing. \nAlthough the type of AI component used  will determine how these criteria should be applied , the AEPD’s \nguide on AI -based data processing (AEPD, 2020[72]) includes examples of the types of information that \nmay be relevant to the data subject: \n• Details about the data used for decision -making beyond just the category, especially information \nregarding the duration of use of the data (how old the data are). \n• Relative importance or weight given to each of the data in the decision-making.  \n• Quality of the training data and the type of models used. \n• Profiling activities conducted and their implications. \n• Error or precision values according to the specific metrics used to measure the validity of the \ninference."]}